# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12wDtSH5-kgCbOdZnHTXy65KRSkapFy9G
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import datasets
import math

# =====================
# 配置
# =====================
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", device)

# 要测试的模型
models = ["distilgpt2", "gpt2", "gpt2-xl", "tiiuae/falcon-7b"]

# 加载数据 (WikiText-2 验证集)
dataset = datasets.load_dataset("wikitext", "wikitext-2-raw-v1", split="validation")

# 拼接成一个长文本
text = "\n\n".join(dataset["text"])

# =====================
# 辅助函数
# =====================

def get_max_length(config):
    """不同模型架构的最大序列长度字段名字不同，这里统一处理"""
    if hasattr(config, "n_positions"):
        return config.n_positions  # GPT2 系列
    elif hasattr(config, "max_position_embeddings"):
        return config.max_position_embeddings  # Falcon, OPT, BLOOM 等
    else:
        return 1024  # fallback 默认值

def compute_perplexity(model_name, max_tokens=None):
    """计算模型在 WikiText2 上的 perplexity"""
    print(f"\n>>> Loading model: {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)

    encodings = tokenizer(text, return_tensors="pt")
    if max_tokens is not None:
        encodings["input_ids"] = encodings["input_ids"][:, :max_tokens]

    max_length = get_max_length(model.config)
    stride = 512

    lls = []
    for i in range(0, encodings.input_ids.size(1), stride):
        begin_loc = max(i + stride - max_length, 0)
        end_loc = i + stride
        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)
        target_ids = input_ids.clone()
        target_ids[:, :-stride] = -100

        with torch.no_grad():
            outputs = model(input_ids, labels=target_ids)
            log_likelihood = outputs.loss * stride

        lls.append(log_likelihood)

    ppl = torch.exp(torch.stack(lls).sum() / end_loc)
    return ppl.item()

# =====================
# 主循环
# =====================

results = {}
for m in models:
    try:
        ppl = compute_perplexity(m, max_tokens=50000)  # 先跑前 50k tokens，避免太慢
        results[m] = ppl
        print(f"{m}: Perplexity = {ppl:.2f}")
    except Exception as e:
        print(f"Error running {m}: {e}")

print("\nFinal results:", results)

import matplotlib.pyplot as plt

# PPL 数据
ppl_data = {
    "distilgpt2": 39.13,
    "gpt2": 25.12,
    "gpt2-xl": 14.53,
    "mistral-7b": 2.07,     # Baseten proxy
    "mixtral-8x7b": 1.95,   # Baseten proxy
}

# 能耗数据 (J/token，已统一)
energy_data = {
    "distilgpt2": 0.000231 * 3.6e6,   # 831 J
    "gpt2": 0.000492 * 3.6e6,         # 1771 J
    "gpt2-xl": 0.000873 * 3.6e6,      # 3143 J
    "mistral-7b": 4.59,               # J
    "mixtral-8x7b": 271.44,           # J
}

# 质量指标 (1/PPL)
quality_data = {m: 1.0/p for m, p in ppl_data.items()}

# 绘制
plt.figure(figsize=(8,6))
for m in ppl_data:
    plt.scatter(energy_data[m], quality_data[m], s=100, label=m)
    plt.text(energy_data[m]*1.1, quality_data[m], m, fontsize=9)

plt.xscale("log")  # 横轴对数，更直观
plt.xlabel("Energy per token (J)", fontsize=12)
plt.ylabel("Quality (1 / Perplexity)", fontsize=12)
plt.title("Energy–Quality Tradeoff", fontsize=14)
plt.grid(True, which="both", linestyle="--", linewidth=0.7)
plt.legend()
plt.tight_layout()
plt.savefig("energy_quality_tradeoff_joules.png", dpi=300)
plt.show()