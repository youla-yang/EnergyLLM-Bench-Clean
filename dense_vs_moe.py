# -*- coding: utf-8 -*-
"""dense vs MoE

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N4rFR2w_pvPnsFqjSF2ckE-N8uYdcC9P
"""

!ln -s /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1 /usr/lib/libnvidia-ml.so.1
!apt-get update
!apt-get install -y libnvidia-ml-dev
!apt-get update
!apt-get install -y python3-pip

!pip install transformers bitsandbytes pynvml accelerate fvcore

import torch, time, csv
import pynvml
from transformers import AutoModelForCausalLM, AutoTokenizer
from fvcore.nn import FlopCountAnalysis

from huggingface_hub import login


!pip install transformers bitsandbytes pynvml accelerate fvcore matplotlib

import os, torch, time, csv, pynvml
from transformers import AutoModelForCausalLM, AutoTokenizer
from fvcore.nn import FlopCountAnalysis
from huggingface_hub import login
import matplotlib.pyplot as plt
import pandas as pd

# ----------------------------
# ç™»å½• HuggingFaceï¼ˆå®‰å…¨ï¼šä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
# åœ¨ Colab å·¦è¾¹æ : âš™ Settings â†’ Secrets â†’ æ·»åŠ  HF_TOKEN
# ç„¶åè¿™é‡Œä¼šè‡ªåŠ¨è¯»å–
# ----------------------------

!pip install transformers bitsandbytes accelerate

import time, csv
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# ----------------------------
# å°è¯•åˆå§‹åŒ– NVML
# ----------------------------
try:
    import pynvml
    pynvml.nvmlInit()
    nvml_available = True
    handle = pynvml.nvmlDeviceGetHandleByIndex(0)
    print("âœ… NVML available, real energy logging enabled")
except:
    nvml_available = False
    print("âš ï¸ NVML not available, fallback to latency measurement")

# ----------------------------
# æµ‹è¯•å‡½æ•°
# ----------------------------
def measure_inference(model_name, seq_len=128, n_samples=3):
    print(f"\n===== Running {model_name} =====")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")
    model.eval()

    total_tokens, total_energy, total_time = 0, 0.0, 0.0

    for i in range(n_samples):
        text = "The quick brown fox jumps over the lazy dog."
        inputs = tokenizer(text, return_tensors="pt").to(model.device)

        if nvml_available:
            power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0

        start = time.time()
        outputs = model.generate(**inputs, max_new_tokens=seq_len)
        duration = time.time() - start

        tokens = outputs.shape[1] - inputs["input_ids"].shape[1]
        total_tokens += tokens
        total_time += duration

        if nvml_available:
            energy = power * duration
            total_energy += energy
            print(f"[{model_name}] Sample {i} | {tokens} tokens | {energy:.2f} J")
        else:
            print(f"[{model_name}] Sample {i} | {tokens} tokens | {duration:.2f} s")

    if nvml_available:
        avg = total_energy / total_tokens
        metric = "J/token"
    else:
        avg = total_time / total_tokens
        metric = "s/token"

    return avg, total_tokens, metric


# ----------------------------
# ä¸»å®éªŒæµç¨‹
# ----------------------------
models = [
    "gpt2",
    "google/gemma-7b",
    "mistralai/Mistral-7B-v0.1",
    "mistralai/Mixtral-8x7B-Instruct-v0.1"
]

results = []
for model in models:
    try:
        avg, tokens, metric = measure_inference(model)
        results.append({
            "Model": model.split("/")[-1],
            "Metric": metric,
            "Value": avg,
            "Tokens": tokens
        })
    except Exception as e:
        print(f"Skipping {model} due to error: {e}")

# ----------------------------
# ä¿å­˜ç»“æœåˆ° CSV
# ----------------------------
with open("dense_vs_moe_results.csv", "w", newline="") as f:
    writer = csv.DictWriter(f, fieldnames=results[0].keys())
    writer.writeheader()
    writer.writerows(results)

print("\n===== Final Results =====")
for r in results:
    print(r)

# å®‰è£…å¿…è¦ä¾èµ–
!pip install -q transformers bitsandbytes accelerate

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from huggingface_hub import login
 #
# ---------
# é…ç½® 4bit é‡åŒ– (èŠ‚çœæ˜¾å­˜)
# ---------
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,                  # ç”¨ 4bit é‡åŒ–
    bnb_4bit_quant_type="nf4",          # æ›´ç¨³å®šçš„é‡åŒ–æ–¹æ¡ˆ
    bnb_4bit_compute_dtype=torch.bfloat16,  # è®¡ç®—ç”¨ bfloat16ï¼ŒA100 æ”¯æŒ
)

# ---------
# åŠ è½½ Mixtral-8x7B Instruct
# ---------
model_name = "mistralai/Mixtral-8x7B-Instruct-v0.1"

print("ğŸš€ å¼€å§‹åŠ è½½æ¨¡å‹ (éœ€è¦å‡ åˆ†é’Ÿ)...")
tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",             # è‡ªåŠ¨åˆ†é… GPU/CPU
    quantization_config=quant_config,
)

# ---------
# ç®€å•æ¨ç†æµ‹è¯•
# ---------
prompt = "Explain the benefits of sparse mixture-of-experts (MoE) models compared to dense transformers."

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

with torch.no_grad():
    output_ids = model.generate(**inputs, max_new_tokens=64)

print("\nğŸ“ è¾“å‡ºç»“æœ:\n")
print(tokenizer.decode(output_ids[0], skip_special_tokens=True))

from huggingface_hub import login
# ---------

!pip install transformers accelerate bitsandbytes pynvml fvcore matplotlib

import time
import torch
import pynvml
import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer

# åˆå§‹åŒ– GPU èƒ½è€—ç›‘æ§
try:
    pynvml.nvmlInit()
    handle = pynvml.nvmlDeviceGetHandleByIndex(0)
    nvml_available = True
    print("âœ… NVML available, energy logging enabled")
except:
    nvml_available = False
    print("âš ï¸ NVML not available, fallback to latency only")


# æµ‹è¯•å‡½æ•°
def measure_inference(model_name, seq_len=128, n_samples=3):
    print(f"\n===== Running {model_name} =====")
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        torch_dtype=torch.float16
    )
    model.eval()

    total_tokens, total_energy, total_time = 0, 0.0, 0.0
    text = "The quick brown fox jumps over the lazy dog."

    for i in range(n_samples):
        inputs = tokenizer(text, return_tensors="pt").to(model.device)

        # é‡‡æ ·åŠŸç‡ï¼šå¼€å§‹å‰
        if nvml_available:
            energy_start = pynvml.nvmlDeviceGetTotalEnergyConsumption(handle)  # mJ

        start = time.time()
        outputs = model.generate(**inputs, max_new_tokens=seq_len)
        duration = time.time() - start

        # é‡‡æ ·åŠŸç‡ï¼šç»“æŸå
        if nvml_available:
            energy_end = pynvml.nvmlDeviceGetTotalEnergyConsumption(handle)  # mJ
            energy = (energy_end - energy_start) / 1000.0  # è½¬ J
            total_energy += energy

        tokens = outputs.shape[1] - inputs["input_ids"].shape[1]
        total_tokens += tokens
        total_time += duration

        if nvml_available:
            print(f"[{model_name}] Sample {i} | {tokens} tokens | {energy:.2f} J | {duration:.2f} s")
        else:
            print(f"[{model_name}] Sample {i} | {tokens} tokens | {duration:.2f} s")

    if nvml_available:
        avg = total_energy / total_tokens
        metric = "J/token"
    else:
        avg = total_time / total_tokens
        metric = "s/token"

    return avg, total_tokens, metric


# Dense vs MoE æ¨¡å‹
models = [
    "gpt2",                                # Dense baseline (å°)
    "mistralai/Mistral-7B-v0.1",           # Dense 7B
    "mistralai/Mixtral-8x7B-Instruct-v0.1" # MoE
]

results = []
for model in models:
    try:
        avg, tokens, metric = measure_inference(model, seq_len=128, n_samples=2)
        results.append({"Model": model.split("/")[-1],
                        "Metric": metric,
                        "Value": avg,
                        "Tokens": tokens})
    except Exception as e:
        print(f"âŒ Skipping {model} due to error: {e}")

# ä¿å­˜ç»“æœ
df = pd.DataFrame(results)
df.to_csv("dense_vs_moe.csv", index=False)
print("\n=== Final Results ===")
print(df)

from google.colab import files
files.download("dense_vs_moe.csv")

import torch, transformers, accelerate, peft, huggingface_hub, pynvml, pandas as pd

print("Torch:", torch.__version__)
print("Transformers:", transformers.__version__)
print("Accelerate:", accelerate.__version__)
print("PEFT:", peft.__version__)
print("HF Hub:", huggingface_hub.__version__)

# æµ‹è¯• NVML æ˜¯å¦å¯ç”¨
try:
    pynvml.nvmlInit()
    handle = pynvml.nvmlDeviceGetHandleByIndex(0)
    print("âœ… NVML OK, GPU:", pynvml.nvmlDeviceGetName(handle))
except:
    print("âš ï¸ NVML not available")

!pip uninstall -y transformers peft
!pip install transformers==4.37.2 peft==0.9.0 accelerate==0.27.2

!pip install --upgrade tokenizers
!pip install --upgrade transformers

!pip install -U transformers==4.41.2 accelerate bitsandbytes pynvml torchmetrics -q
!python3 -m ensurepip --upgrade
!python3 -m pip install --upgrade pip

!pip install transformers bitsandbytes accelerate pynvml torchmetrics -q

import torch, time, math
from transformers import AutoModelForCausalLM, AutoTokenizer
import pynvml

# åˆå§‹åŒ– NVMLï¼ˆGPU èƒ½è€—è®°å½•ï¼‰
pynvml.nvmlInit()
handle = pynvml.nvmlDeviceGetHandleByIndex(0)

def measure_inference(model_name, seq_len=64, text="The quick brown fox jumps over the lazy dog."):
    print(f"\n===== Running {model_name} =====")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")
    model.eval()

    inputs = tokenizer(text, return_tensors="pt").to(model.device)

    # è®°å½•åŠŸç‡
    power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # W
    start = time.time()
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=seq_len)
    duration = time.time() - start

    tokens = outputs.shape[1] - inputs["input_ids"].shape[1]
    energy = power * duration
    j_per_token = energy / tokens

    # ---------- è®¡ç®— PPL ----------
    labels = inputs["input_ids"]
    with torch.no_grad():
        loss = model(**inputs, labels=labels).loss.item()
    ppl = math.exp(loss)

    print(f"â± Time: {duration:.2f}s | âš¡ {energy:.2f} J | {j_per_token:.4f} J/token | PPL={ppl:.2f}")
    return {"Model": model_name.split("/")[-1], "J/token": j_per_token, "PPL": ppl}


# --------------------------
# å®éªŒæ¨¡å‹åˆ—è¡¨ï¼ˆDense vs MoEï¼‰
# --------------------------
models = [
    "gpt2",                                # Dense baseline
    "mistralai/Mistral-7B-v0.1",           # Dense 7B
    "mistralai/Mixtral-8x7B-Instruct-v0.1" # MoE
]

results = []
for m in models:
    try:
        res = measure_inference(m)
        results.append(res)
    except Exception as e:
        print(f"âŒ Skipping {m}: {e}")

import pandas as pd
df = pd.DataFrame(results)
print("\n===== Final Results =====")
print(df)

!pip install transformers accelerate bitsandbytes pynvml matplotlib datasets evaluate -q

import torch, time
import pynvml
import matplotlib.pyplot as plt
import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer
import evaluate

# ========== 1. åˆå§‹åŒ– NVMLï¼ˆèƒ½è€—ï¼‰ ==========
try:
    pynvml.nvmlInit()
    handle = pynvml.nvmlDeviceGetHandleByIndex(0)
    nvml_available = True
    print("âœ… NVML available, will log energy usage")
except:
    nvml_available = False
    print("âš ï¸ NVML not available, fallback to latency only")

# ========== 2. ç»Ÿä¸€æµ‹è¯•å‡½æ•° ==========
def measure_model(model_name, seq_len=128, n_samples=3, dataset="wikitext", split="test[:1%]"):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        torch_dtype=torch.float16
    )
    model.eval()

    total_tokens, total_time, total_energy = 0, 0.0, 0.0

    for i in range(n_samples):
        text = "The quick brown fox jumps over the lazy dog."
        inputs = tokenizer(text, return_tensors="pt").to(model.device)

        # è¯»å– GPU åŠŸè€—
        if nvml_available:
            power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # W

        start = time.time()
        outputs = model.generate(**inputs, max_new_tokens=seq_len)
        duration = time.time() - start

        tokens = outputs.shape[1] - inputs["input_ids"].shape[1]
        total_tokens += tokens
        total_time += duration

        if nvml_available:
            total_energy += power * duration

    # Perplexity (PPL)
    ppl_metric = evaluate.load("perplexity", module_type="metric")
    test_text = "The quick brown fox jumps over the lazy dog."
    ppl = ppl_metric.compute(model_id=model_name, add_start_token=True, input_texts=[test_text])["perplexities"][0]

    avg_s_token = total_time / total_tokens
    avg_j_token = (total_energy / total_tokens) if nvml_available else None

    return {
        "Model": model_name.split("/")[-1],
        "s/token": avg_s_token,
        "J/token": avg_j_token,
        "PPL": ppl
    }

# ========== 3. è¿è¡Œå®éªŒ ==========
models = [
    "gpt2",                          # Dense small
    "meta-llama/Llama-2-7b-hf",      # Dense large
    "mistralai/Mixtral-8x7B-Instruct-v0.1"  # MoE
]

results = []
for m in models:
    try:
        res = measure_model(m)
        results.append(res)
        print(res)
    except Exception as e:
        print(f"âš ï¸ Skipping {m}: {e}")

df = pd.DataFrame(results)
print(df)

# ========== 4. å¯è§†åŒ– ==========
plt.figure(figsize=(8,5))
df.plot(x="Model", y="J/token", kind="bar", legend=False)
plt.ylabel("Energy (J/token)")
plt.title("Dense vs MoE Energy Efficiency")
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(6,5))
plt.scatter(df["PPL"], df["J/token"], s=80, c="blue")
for i,row in df.iterrows():
    plt.text(row["PPL"], row["J/token"], row["Model"], fontsize=9)
plt.xlabel("Perplexity (PPL)")
plt.ylabel("Energy (J/token)")
plt.title("Pareto Frontier: Energy vs Quality")
plt.grid(True)
plt.show()

!pip install transformers accelerate bitsandbytes pynvml matplotlib -q

import torch, time
import pynvml
import matplotlib.pyplot as plt
import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# ========== 1. åˆå§‹åŒ– NVML ==========
try:
    pynvml.nvmlInit()
    handle = pynvml.nvmlDeviceGetHandleByIndex(0)
    nvml_available = True
    print("âœ… NVML available, will log energy usage")
except:
    nvml_available = False
    print("âš ï¸ NVML not available, fallback to latency only")

# ========== 2. PPL è®¡ç®—å‡½æ•° ==========
def compute_perplexity(model, tokenizer, text="The quick brown fox jumps over the lazy dog."):
    encodings = tokenizer(text, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model(input_ids=encodings.input_ids, labels=encodings.input_ids)
        neg_log_likelihood = outputs.loss
    ppl = torch.exp(neg_log_likelihood).item()
    return ppl

# ========== 3. ä¸»æµ‹è¯•å‡½æ•° ==========
def measure_model(model_name, seq_len=64, n_samples=2):
    print(f"\n===== Running {model_name} =====")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        torch_dtype=torch.float16
    )
    model.eval()

    total_tokens, total_time, total_energy = 0, 0.0, 0.0
    text = "The quick brown fox jumps over the lazy dog."

    for i in range(n_samples):
        inputs = tokenizer(text, return_tensors="pt").to(model.device)

        if nvml_available:
            power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # W

        start = time.time()
        outputs = model.generate(**inputs, max_new_tokens=seq_len)
        duration = time.time() - start

        tokens = outputs.shape[1] - inputs["input_ids"].shape[1]
        total_tokens += tokens
        total_time += duration

        if nvml_available:
            total_energy += power * duration

    avg_s_token = total_time / total_tokens
    avg_j_token = (total_energy / total_tokens) if nvml_available else None
    ppl = compute_perplexity(model, tokenizer, text)

    return {
        "Model": model_name.split("/")[-1],
        "s/token": avg_s_token,
        "J/token": avg_j_token,
        "PPL": ppl
    }

# ========== 4. Dense vs MoE æ¨¡å‹ ==========
models = [
    "gpt2",                          # Dense small
    "mistralai/Mistral-7B-v0.1",     # Dense 7B
    "mistralai/Mixtral-8x7B-Instruct-v0.1"  # MoE
]

results = []
for m in models:
    try:
        res = measure_model(m)
        results.append(res)
        print(res)
    except Exception as e:
        print(f"âš ï¸ Skipping {m}: {e}")

df = pd.DataFrame(results)
print("\n===== Final Results =====")
print(df)

# ========== 5. å¯è§†åŒ– ==========
# Bar chart
plt.figure(figsize=(8,5))
df.plot(x="Model", y="J/token", kind="bar", legend=False)
plt.ylabel("Energy (J/token)" if nvml_available else "Latency (s/token)")
plt.title("Dense vs MoE Energy Efficiency")
plt.xticks(rotation=45)
plt.show()

# Pareto frontier
plt.figure(figsize=(6,5))
plt.scatter(df["PPL"], df["J/token"], s=80, c="blue")
for i,row in df.iterrows():
    plt.text(row["PPL"], row["J/token"], row["Model"], fontsize=9)
plt.xlabel("Perplexity (PPL)")
plt.ylabel("Energy (J/token)" if nvml_available else "Latency (s/token)")
plt.title("Pareto Frontier: Energy vs Quality")
plt.grid(True)
plt.show()

!pip install transformers datasets bitsandbytes accelerate pynvml matplotlib seaborn -q

import os, time, csv
import numpy as np
import torch
import pynvml
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
from torch.utils.data import DataLoader
from tqdm import tqdm

# ----------------------------
# åˆå§‹åŒ– NVML
# ----------------------------
try:
    pynvml.nvmlInit()
    nvml_available = True
    handle = pynvml.nvmlDeviceGetHandleByIndex(0)
    print("âœ… NVML available, real energy logging enabled")
except:
    nvml_available = False
    print("âš ï¸ NVML not available, fallback to latency measurement")

# ----------------------------
# æ•°æ®é›† (WikiText-2 Perplexity)
# ----------------------------
dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="test")
texts = [x["text"] for x in dataset if len(x["text"].strip()) > 0]
sample_texts = texts[:200]   # é™åˆ¶å­é›†é¿å…å¤ªæ…¢

# ----------------------------
# æµ‹é‡å‡½æ•°
# ----------------------------
def evaluate_model(model_name, seq_len=128, n_runs=3):
    print(f"\n===== Running {model_name} =====")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        torch_dtype=torch.float16
    )
    model.eval()

    all_j_per_token, all_ppl = [], []

    for run in range(n_runs):
        print(f" Run {run+1}/{n_runs} ...")
        total_tokens, total_energy, total_time, total_loss = 0, 0.0, 0.0, 0.0

        for text in tqdm(sample_texts[:50]):  # åªè¯„ä¼° 50 æ®µæ–‡æœ¬ï¼ŒåŠ å¿«é€Ÿåº¦
            inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=seq_len).to(model.device)

            # ---------- èƒ½è€— ----------
            if nvml_available:
                power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0
            start = time.time()

            with torch.no_grad():
                outputs = model(**inputs, labels=inputs["input_ids"])
                loss = outputs.loss

            duration = time.time() - start
            tokens = inputs["input_ids"].numel()
            total_tokens += tokens
            total_time += duration
            total_loss += loss.item() * tokens

            if nvml_available:
                energy = power * duration
                total_energy += energy

        # ---------- ç»Ÿè®¡ ----------
        ppl = np.exp(total_loss / total_tokens)
        if nvml_available:
            j_per_token = total_energy / total_tokens
        else:
            j_per_token = total_time / total_tokens  # proxy

        print(f"  >> Run {run+1} - J/token: {j_per_token:.3f}, PPL: {ppl:.2f}")
        all_j_per_token.append(j_per_token)
        all_ppl.append(ppl)

    return {
        "Model": model_name.split("/")[-1],
        "J/token (mean)": np.mean(all_j_per_token),
        "J/token (std)": np.std(all_j_per_token),
        "PPL (mean)": np.mean(all_ppl),
        "PPL (std)": np.std(all_ppl)
    }

# ----------------------------
# å®éªŒæ¨¡å‹åˆ—è¡¨
# ----------------------------
models = [
    "gpt2",                              # Dense (å°)
    "mistralai/Mistral-7B-v0.1",         # Dense (å¤§)
    "mistralai/Mixtral-8x7B-Instruct-v0.1"  # MoE
]

results = []
for m in models:
    try:
        res = evaluate_model(m, n_runs=3)
        results.append(res)
    except Exception as e:
        print(f"âš ï¸ Skipping {m} due to error: {e}")

# ----------------------------
# ä¿å­˜ç»“æœ
# ----------------------------
import pandas as pd
df = pd.DataFrame(results)
df.to_csv("dense_vs_moe_benchmark.csv", index=False)
print("\n===== Final Results =====")
print(df)

# ----------------------------
# ä½œå›¾
# ----------------------------
sns.set(style="whitegrid", font_scale=1.2)

# Bar Chart
plt.figure(figsize=(8,5))
sns.barplot(data=df, x="Model", y="J/token (mean)", palette="muted", capsize=0.2)
plt.title("Energy Efficiency (J/token)")
plt.ylabel("J/token (lower is better)")
plt.savefig("bar_chart.png")
plt.show()

# Pareto frontier (J/token vs PPL)
plt.figure(figsize=(7,5))
plt.errorbar(df["PPL (mean)"], df["J/token (mean)"],
             xerr=df["PPL (std)"], yerr=df["J/token (std)"],
             fmt="o", capsize=5)
for i, row in df.iterrows():
    plt.text(row["PPL (mean)"]*1.05, row["J/token (mean)"], row["Model"])
plt.xlabel("Perplexity (PPL)")
plt.ylabel("Energy (J/token)")
plt.title("Pareto Frontier: Efficiency vs Accuracy")
plt.savefig("pareto_frontier.png")
plt.show()