# -*- coding: utf-8 -*-
"""dense vs MoE

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N4rFR2w_pvPnsFqjSF2ckE-N8uYdcC9P
"""

!ln -s /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1 /usr/lib/libnvidia-ml.so.1
!apt-get update
!apt-get install -y libnvidia-ml-dev
!apt-get update
!apt-get install -y python3-pip

!pip install transformers bitsandbytes pynvml accelerate fvcore

import torch, time, csv
import pynvml
from transformers import AutoModelForCausalLM, AutoTokenizer
from fvcore.nn import FlopCountAnalysis

from huggingface_hub import login


!pip install transformers bitsandbytes pynvml accelerate fvcore matplotlib

import os, torch, time, csv, pynvml
from transformers import AutoModelForCausalLM, AutoTokenizer
from fvcore.nn import FlopCountAnalysis
from huggingface_hub import login
import matplotlib.pyplot as plt
import pandas as pd

# ----------------------------
# 登录 HuggingFace（安全：从环境变量读取）
# 在 Colab 左边栏: ⚙ Settings → Secrets → 添加 HF_TOKEN
# 然后这里会自动读取
# ----------------------------

!pip install transformers bitsandbytes accelerate

import time, csv
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# ----------------------------
# 尝试初始化 NVML
# ----------------------------
try:
    import pynvml
    pynvml.nvmlInit()
    nvml_available = True
    handle = pynvml.nvmlDeviceGetHandleByIndex(0)
    print("✅ NVML available, real energy logging enabled")
except:
    nvml_available = False
    print("⚠️ NVML not available, fallback to latency measurement")

# ----------------------------
# 测试函数
# ----------------------------
def measure_inference(model_name, seq_len=128, n_samples=3):
    print(f"\n===== Running {model_name} =====")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")
    model.eval()

    total_tokens, total_energy, total_time = 0, 0.0, 0.0

    for i in range(n_samples):
        text = "The quick brown fox jumps over the lazy dog."
        inputs = tokenizer(text, return_tensors="pt").to(model.device)

        if nvml_available:
            power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0

        start = time.time()
        outputs = model.generate(**inputs, max_new_tokens=seq_len)
        duration = time.time() - start

        tokens = outputs.shape[1] - inputs["input_ids"].shape[1]
        total_tokens += tokens
        total_time += duration

        if nvml_available:
            energy = power * duration
            total_energy += energy
            print(f"[{model_name}] Sample {i} | {tokens} tokens | {energy:.2f} J")
        else:
            print(f"[{model_name}] Sample {i} | {tokens} tokens | {duration:.2f} s")

    if nvml_available:
        avg = total_energy / total_tokens
        metric = "J/token"
    else:
        avg = total_time / total_tokens
        metric = "s/token"

    return avg, total_tokens, metric


# ----------------------------
# 主实验流程
# ----------------------------
models = [
    "gpt2",
    "google/gemma-7b",
    "mistralai/Mistral-7B-v0.1",
    "mistralai/Mixtral-8x7B-Instruct-v0.1"
]

results = []
for model in models:
    try:
        avg, tokens, metric = measure_inference(model)
        results.append({
            "Model": model.split("/")[-1],
            "Metric": metric,
            "Value": avg,
            "Tokens": tokens
        })
    except Exception as e:
        print(f"Skipping {model} due to error: {e}")

# ----------------------------
# 保存结果到 CSV
# ----------------------------
with open("dense_vs_moe_results.csv", "w", newline="") as f:
    writer = csv.DictWriter(f, fieldnames=results[0].keys())
    writer.writeheader()
    writer.writerows(results)

print("\n===== Final Results =====")
for r in results:
    print(r)

# 安装必要依赖
!pip install -q transformers bitsandbytes accelerate

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from huggingface_hub import login
 #
# ---------
# 配置 4bit 量化 (节省显存)
# ---------
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,                  # 用 4bit 量化
    bnb_4bit_quant_type="nf4",          # 更稳定的量化方案
    bnb_4bit_compute_dtype=torch.bfloat16,  # 计算用 bfloat16，A100 支持
)

# ---------
# 加载 Mixtral-8x7B Instruct
# ---------
model_name = "mistralai/Mixtral-8x7B-Instruct-v0.1"

print("🚀 开始加载模型 (需要几分钟)...")
tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",             # 自动分配 GPU/CPU
    quantization_config=quant_config,
)

# ---------
# 简单推理测试
# ---------
prompt = "Explain the benefits of sparse mixture-of-experts (MoE) models compared to dense transformers."

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

with torch.no_grad():
    output_ids = model.generate(**inputs, max_new_tokens=64)

print("\n📝 输出结果:\n")
print(tokenizer.decode(output_ids[0], skip_special_tokens=True))

from huggingface_hub import login
# ---------

!pip install transformers accelerate bitsandbytes pynvml fvcore matplotlib

import time
import torch
import pynvml
import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer

# 初始化 GPU 能耗监控
try:
    pynvml.nvmlInit()
    handle = pynvml.nvmlDeviceGetHandleByIndex(0)
    nvml_available = True
    print("✅ NVML available, energy logging enabled")
except:
    nvml_available = False
    print("⚠️ NVML not available, fallback to latency only")


# 测试函数
def measure_inference(model_name, seq_len=128, n_samples=3):
    print(f"\n===== Running {model_name} =====")
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # 加载模型
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        torch_dtype=torch.float16
    )
    model.eval()

    total_tokens, total_energy, total_time = 0, 0.0, 0.0
    text = "The quick brown fox jumps over the lazy dog."

    for i in range(n_samples):
        inputs = tokenizer(text, return_tensors="pt").to(model.device)

        # 采样功率：开始前
        if nvml_available:
            energy_start = pynvml.nvmlDeviceGetTotalEnergyConsumption(handle)  # mJ

        start = time.time()
        outputs = model.generate(**inputs, max_new_tokens=seq_len)
        duration = time.time() - start

        # 采样功率：结束后
        if nvml_available:
            energy_end = pynvml.nvmlDeviceGetTotalEnergyConsumption(handle)  # mJ
            energy = (energy_end - energy_start) / 1000.0  # 转 J
            total_energy += energy

        tokens = outputs.shape[1] - inputs["input_ids"].shape[1]
        total_tokens += tokens
        total_time += duration

        if nvml_available:
            print(f"[{model_name}] Sample {i} | {tokens} tokens | {energy:.2f} J | {duration:.2f} s")
        else:
            print(f"[{model_name}] Sample {i} | {tokens} tokens | {duration:.2f} s")

    if nvml_available:
        avg = total_energy / total_tokens
        metric = "J/token"
    else:
        avg = total_time / total_tokens
        metric = "s/token"

    return avg, total_tokens, metric


# Dense vs MoE 模型
models = [
    "gpt2",                                # Dense baseline (小)
    "mistralai/Mistral-7B-v0.1",           # Dense 7B
    "mistralai/Mixtral-8x7B-Instruct-v0.1" # MoE
]

results = []
for model in models:
    try:
        avg, tokens, metric = measure_inference(model, seq_len=128, n_samples=2)
        results.append({"Model": model.split("/")[-1],
                        "Metric": metric,
                        "Value": avg,
                        "Tokens": tokens})
    except Exception as e:
        print(f"❌ Skipping {model} due to error: {e}")

# 保存结果
df = pd.DataFrame(results)
df.to_csv("dense_vs_moe.csv", index=False)
print("\n=== Final Results ===")
print(df)

from google.colab import files
files.download("dense_vs_moe.csv")

import torch, transformers, accelerate, peft, huggingface_hub, pynvml, pandas as pd

print("Torch:", torch.__version__)
print("Transformers:", transformers.__version__)
print("Accelerate:", accelerate.__version__)
print("PEFT:", peft.__version__)
print("HF Hub:", huggingface_hub.__version__)

# 测试 NVML 是否可用
try:
    pynvml.nvmlInit()
    handle = pynvml.nvmlDeviceGetHandleByIndex(0)
    print("✅ NVML OK, GPU:", pynvml.nvmlDeviceGetName(handle))
except:
    print("⚠️ NVML not available")

!pip uninstall -y transformers peft
!pip install transformers==4.37.2 peft==0.9.0 accelerate==0.27.2

!pip install --upgrade tokenizers
!pip install --upgrade transformers

!pip install -U transformers==4.41.2 accelerate bitsandbytes pynvml torchmetrics -q
!python3 -m ensurepip --upgrade
!python3 -m pip install --upgrade pip

!pip install transformers bitsandbytes accelerate pynvml torchmetrics -q

import torch, time, math
from transformers import AutoModelForCausalLM, AutoTokenizer
import pynvml

# 初始化 NVML（GPU 能耗记录）
pynvml.nvmlInit()
handle = pynvml.nvmlDeviceGetHandleByIndex(0)

def measure_inference(model_name, seq_len=64, text="The quick brown fox jumps over the lazy dog."):
    print(f"\n===== Running {model_name} =====")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")
    model.eval()

    inputs = tokenizer(text, return_tensors="pt").to(model.device)

    # 记录功率
    power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # W
    start = time.time()
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=seq_len)
    duration = time.time() - start

    tokens = outputs.shape[1] - inputs["input_ids"].shape[1]
    energy = power * duration
    j_per_token = energy / tokens

    # ---------- 计算 PPL ----------
    labels = inputs["input_ids"]
    with torch.no_grad():
        loss = model(**inputs, labels=labels).loss.item()
    ppl = math.exp(loss)

    print(f"⏱ Time: {duration:.2f}s | ⚡ {energy:.2f} J | {j_per_token:.4f} J/token | PPL={ppl:.2f}")
    return {"Model": model_name.split("/")[-1], "J/token": j_per_token, "PPL": ppl}


# --------------------------
# 实验模型列表（Dense vs MoE）
# --------------------------
models = [
    "gpt2",                                # Dense baseline
    "mistralai/Mistral-7B-v0.1",           # Dense 7B
    "mistralai/Mixtral-8x7B-Instruct-v0.1" # MoE
]

results = []
for m in models:
    try:
        res = measure_inference(m)
        results.append(res)
    except Exception as e:
        print(f"❌ Skipping {m}: {e}")

import pandas as pd
df = pd.DataFrame(results)
print("\n===== Final Results =====")
print(df)

!pip install transformers accelerate bitsandbytes pynvml matplotlib datasets evaluate -q

import torch, time
import pynvml
import matplotlib.pyplot as plt
import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer
import evaluate

# ========== 1. 初始化 NVML（能耗） ==========
try:
    pynvml.nvmlInit()
    handle = pynvml.nvmlDeviceGetHandleByIndex(0)
    nvml_available = True
    print("✅ NVML available, will log energy usage")
except:
    nvml_available = False
    print("⚠️ NVML not available, fallback to latency only")

# ========== 2. 统一测试函数 ==========
def measure_model(model_name, seq_len=128, n_samples=3, dataset="wikitext", split="test[:1%]"):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        torch_dtype=torch.float16
    )
    model.eval()

    total_tokens, total_time, total_energy = 0, 0.0, 0.0

    for i in range(n_samples):
        text = "The quick brown fox jumps over the lazy dog."
        inputs = tokenizer(text, return_tensors="pt").to(model.device)

        # 读取 GPU 功耗
        if nvml_available:
            power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # W

        start = time.time()
        outputs = model.generate(**inputs, max_new_tokens=seq_len)
        duration = time.time() - start

        tokens = outputs.shape[1] - inputs["input_ids"].shape[1]
        total_tokens += tokens
        total_time += duration

        if nvml_available:
            total_energy += power * duration

    # Perplexity (PPL)
    ppl_metric = evaluate.load("perplexity", module_type="metric")
    test_text = "The quick brown fox jumps over the lazy dog."
    ppl = ppl_metric.compute(model_id=model_name, add_start_token=True, input_texts=[test_text])["perplexities"][0]

    avg_s_token = total_time / total_tokens
    avg_j_token = (total_energy / total_tokens) if nvml_available else None

    return {
        "Model": model_name.split("/")[-1],
        "s/token": avg_s_token,
        "J/token": avg_j_token,
        "PPL": ppl
    }

# ========== 3. 运行实验 ==========
models = [
    "gpt2",                          # Dense small
    "meta-llama/Llama-2-7b-hf",      # Dense large
    "mistralai/Mixtral-8x7B-Instruct-v0.1"  # MoE
]

results = []
for m in models:
    try:
        res = measure_model(m)
        results.append(res)
        print(res)
    except Exception as e:
        print(f"⚠️ Skipping {m}: {e}")

df = pd.DataFrame(results)
print(df)

# ========== 4. 可视化 ==========
plt.figure(figsize=(8,5))
df.plot(x="Model", y="J/token", kind="bar", legend=False)
plt.ylabel("Energy (J/token)")
plt.title("Dense vs MoE Energy Efficiency")
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(6,5))
plt.scatter(df["PPL"], df["J/token"], s=80, c="blue")
for i,row in df.iterrows():
    plt.text(row["PPL"], row["J/token"], row["Model"], fontsize=9)
plt.xlabel("Perplexity (PPL)")
plt.ylabel("Energy (J/token)")
plt.title("Pareto Frontier: Energy vs Quality")
plt.grid(True)
plt.show()

!pip install transformers accelerate bitsandbytes pynvml matplotlib -q

import torch, time
import pynvml
import matplotlib.pyplot as plt
import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# ========== 1. 初始化 NVML ==========
try:
    pynvml.nvmlInit()
    handle = pynvml.nvmlDeviceGetHandleByIndex(0)
    nvml_available = True
    print("✅ NVML available, will log energy usage")
except:
    nvml_available = False
    print("⚠️ NVML not available, fallback to latency only")

# ========== 2. PPL 计算函数 ==========
def compute_perplexity(model, tokenizer, text="The quick brown fox jumps over the lazy dog."):
    encodings = tokenizer(text, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model(input_ids=encodings.input_ids, labels=encodings.input_ids)
        neg_log_likelihood = outputs.loss
    ppl = torch.exp(neg_log_likelihood).item()
    return ppl

# ========== 3. 主测试函数 ==========
def measure_model(model_name, seq_len=64, n_samples=2):
    print(f"\n===== Running {model_name} =====")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        torch_dtype=torch.float16
    )
    model.eval()

    total_tokens, total_time, total_energy = 0, 0.0, 0.0
    text = "The quick brown fox jumps over the lazy dog."

    for i in range(n_samples):
        inputs = tokenizer(text, return_tensors="pt").to(model.device)

        if nvml_available:
            power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # W

        start = time.time()
        outputs = model.generate(**inputs, max_new_tokens=seq_len)
        duration = time.time() - start

        tokens = outputs.shape[1] - inputs["input_ids"].shape[1]
        total_tokens += tokens
        total_time += duration

        if nvml_available:
            total_energy += power * duration

    avg_s_token = total_time / total_tokens
    avg_j_token = (total_energy / total_tokens) if nvml_available else None
    ppl = compute_perplexity(model, tokenizer, text)

    return {
        "Model": model_name.split("/")[-1],
        "s/token": avg_s_token,
        "J/token": avg_j_token,
        "PPL": ppl
    }

# ========== 4. Dense vs MoE 模型 ==========
models = [
    "gpt2",                          # Dense small
    "mistralai/Mistral-7B-v0.1",     # Dense 7B
    "mistralai/Mixtral-8x7B-Instruct-v0.1"  # MoE
]

results = []
for m in models:
    try:
        res = measure_model(m)
        results.append(res)
        print(res)
    except Exception as e:
        print(f"⚠️ Skipping {m}: {e}")

df = pd.DataFrame(results)
print("\n===== Final Results =====")
print(df)

# ========== 5. 可视化 ==========
# Bar chart
plt.figure(figsize=(8,5))
df.plot(x="Model", y="J/token", kind="bar", legend=False)
plt.ylabel("Energy (J/token)" if nvml_available else "Latency (s/token)")
plt.title("Dense vs MoE Energy Efficiency")
plt.xticks(rotation=45)
plt.show()

# Pareto frontier
plt.figure(figsize=(6,5))
plt.scatter(df["PPL"], df["J/token"], s=80, c="blue")
for i,row in df.iterrows():
    plt.text(row["PPL"], row["J/token"], row["Model"], fontsize=9)
plt.xlabel("Perplexity (PPL)")
plt.ylabel("Energy (J/token)" if nvml_available else "Latency (s/token)")
plt.title("Pareto Frontier: Energy vs Quality")
plt.grid(True)
plt.show()

!pip install transformers datasets bitsandbytes accelerate pynvml matplotlib seaborn -q

import os, time, csv
import numpy as np
import torch
import pynvml
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
from torch.utils.data import DataLoader
from tqdm import tqdm

# ----------------------------
# 初始化 NVML
# ----------------------------
try:
    pynvml.nvmlInit()
    nvml_available = True
    handle = pynvml.nvmlDeviceGetHandleByIndex(0)
    print("✅ NVML available, real energy logging enabled")
except:
    nvml_available = False
    print("⚠️ NVML not available, fallback to latency measurement")

# ----------------------------
# 数据集 (WikiText-2 Perplexity)
# ----------------------------
dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="test")
texts = [x["text"] for x in dataset if len(x["text"].strip()) > 0]
sample_texts = texts[:200]   # 限制子集避免太慢

# ----------------------------
# 测量函数
# ----------------------------
def evaluate_model(model_name, seq_len=128, n_runs=3):
    print(f"\n===== Running {model_name} =====")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        torch_dtype=torch.float16
    )
    model.eval()

    all_j_per_token, all_ppl = [], []

    for run in range(n_runs):
        print(f" Run {run+1}/{n_runs} ...")
        total_tokens, total_energy, total_time, total_loss = 0, 0.0, 0.0, 0.0

        for text in tqdm(sample_texts[:50]):  # 只评估 50 段文本，加快速度
            inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=seq_len).to(model.device)

            # ---------- 能耗 ----------
            if nvml_available:
                power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0
            start = time.time()

            with torch.no_grad():
                outputs = model(**inputs, labels=inputs["input_ids"])
                loss = outputs.loss

            duration = time.time() - start
            tokens = inputs["input_ids"].numel()
            total_tokens += tokens
            total_time += duration
            total_loss += loss.item() * tokens

            if nvml_available:
                energy = power * duration
                total_energy += energy

        # ---------- 统计 ----------
        ppl = np.exp(total_loss / total_tokens)
        if nvml_available:
            j_per_token = total_energy / total_tokens
        else:
            j_per_token = total_time / total_tokens  # proxy

        print(f"  >> Run {run+1} - J/token: {j_per_token:.3f}, PPL: {ppl:.2f}")
        all_j_per_token.append(j_per_token)
        all_ppl.append(ppl)

    return {
        "Model": model_name.split("/")[-1],
        "J/token (mean)": np.mean(all_j_per_token),
        "J/token (std)": np.std(all_j_per_token),
        "PPL (mean)": np.mean(all_ppl),
        "PPL (std)": np.std(all_ppl)
    }

# ----------------------------
# 实验模型列表
# ----------------------------
models = [
    "gpt2",                              # Dense (小)
    "mistralai/Mistral-7B-v0.1",         # Dense (大)
    "mistralai/Mixtral-8x7B-Instruct-v0.1"  # MoE
]

results = []
for m in models:
    try:
        res = evaluate_model(m, n_runs=3)
        results.append(res)
    except Exception as e:
        print(f"⚠️ Skipping {m} due to error: {e}")

# ----------------------------
# 保存结果
# ----------------------------
import pandas as pd
df = pd.DataFrame(results)
df.to_csv("dense_vs_moe_benchmark.csv", index=False)
print("\n===== Final Results =====")
print(df)

# ----------------------------
# 作图
# ----------------------------
sns.set(style="whitegrid", font_scale=1.2)

# Bar Chart
plt.figure(figsize=(8,5))
sns.barplot(data=df, x="Model", y="J/token (mean)", palette="muted", capsize=0.2)
plt.title("Energy Efficiency (J/token)")
plt.ylabel("J/token (lower is better)")
plt.savefig("bar_chart.png")
plt.show()

# Pareto frontier (J/token vs PPL)
plt.figure(figsize=(7,5))
plt.errorbar(df["PPL (mean)"], df["J/token (mean)"],
             xerr=df["PPL (std)"], yerr=df["J/token (std)"],
             fmt="o", capsize=5)
for i, row in df.iterrows():
    plt.text(row["PPL (mean)"]*1.05, row["J/token (mean)"], row["Model"])
plt.xlabel("Perplexity (PPL)")
plt.ylabel("Energy (J/token)")
plt.title("Pareto Frontier: Efficiency vs Accuracy")
plt.savefig("pareto_frontier.png")
plt.show()