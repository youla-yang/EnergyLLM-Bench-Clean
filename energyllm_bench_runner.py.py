# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LsEM6_KbkG2PFaMfbFH9GxhjOAAGMk4Q
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
# æŒ‚è½½ Google Drive
from google.colab import drive
drive.mount('/content/drive')

# åˆ‡æ¢åˆ°é¡¹ç›®ç›®å½• (è¯·å…ˆæŠŠä½ çš„ codecarbon-master ä¸Šä¼ åˆ° Google Drive)
# %cd /content/drive/MyDrive/codecarbon-master

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆå¯é€‰ï¼ŒColab è‡ªå¸¦ç¯å¢ƒä¸€èˆ¬å¤Ÿç”¨ï¼‰
!pip install --upgrade pip
!pip install torch transformers codecarbon

import os

LOG_FILE = "logs/test_llm.jsonl"
os.makedirs("logs", exist_ok=True)

# å®šä¹‰è¦è·‘çš„æ¨¡å‹
gpt2_models = ["distilgpt2", "gpt2", "gpt2-medium", "gpt2-large", "gpt2-xl"]
llama_models = ["meta-llama/Llama-2-7b-hf"]   # âš ï¸ éœ€è¦ HuggingFace æƒé™
falcon_models = ["tiiuae/falcon-7b"]

def run_cmd(cmd):
    print(f"\n===== Running: {cmd} =====")
    os.system(cmd)

# ----------------------------
# GPT-2 ç³»åˆ—
# ----------------------------
for model in gpt2_models:
    run_cmd(f"python test_llm.py --mode infer --model_name {model} --repeats 5 --seq_len 128 --log_file {LOG_FILE}")
    run_cmd(f"python test_llm.py --mode infer --model_name {model} --repeats 5 --seq_len 512 --log_file {LOG_FILE}")
    run_cmd(f"python test_llm.py --mode train --model_name {model} --batch_size 2 --seq_len 128 --log_file {LOG_FILE}")

# ----------------------------
# LLaMA-2-7B
# ----------------------------
for model in llama_models:
    run_cmd(f"python test_llm.py --mode infer --model_name {model} --repeats 3 --seq_len 128 --log_file {LOG_FILE}")
    run_cmd(f"python test_llm.py --mode infer --model_name {model} --repeats 3 --seq_len 512 --log_file {LOG_FILE}")
    run_cmd(f"python test_llm.py --mode train --model_name {model} --batch_size 1 --seq_len 128 --log_file {LOG_FILE}")

# ----------------------------
# Falcon-7B
# ----------------------------
for model in falcon_models:
    run_cmd(f"python test_llm.py --mode infer --model_name {model} --repeats 3 --seq_len 128 --log_file {LOG_FILE}")
    run_cmd(f"python test_llm.py --mode infer --model_name {model} --repeats 3 --seq_len 512 --log_file {LOG_FILE}")
    run_cmd(f"python test_llm.py --mode train --model_name {model} --batch_size 1 --seq_len 128 --log_file {LOG_FILE}")

print("\nâœ… å…¨éƒ¨å®éªŒå®Œæˆï¼Œç»“æœå·²å†™å…¥ logs/test_llm.jsonl å’Œ leaderboard.csv")

import os

LOG_FILE = "logs/test_llm.jsonl"
os.makedirs("logs", exist_ok=True)

# ----------------------------
# è¦è·‘çš„ GPU æ¨¡å‹
# ----------------------------
gpu_models = {
    # ä¸­ç­‰è§„æ¨¡ï¼ˆ~1Bï¼‰
    "facebook/opt-1.3b": {"batch": 2},
    "EleutherAI/pythia-1b": {"batch": 2},
    "bigscience/bloom-1b1": {"batch": 1},

    # å¤§æ¨¡å‹ï¼ˆ7B çº§ï¼‰
    "mistralai/Mistral-7B-v0.1": {"batch": 1},
    "Qwen/Qwen-7B": {"batch": 1},
    "tiiuae/falcon-7b": {"batch": 1},
    "google/gemma-7b": {"batch": 1},  # å¦‚æœ HF æˆæƒå¯ç”¨
}

def run_cmd(cmd):
    print(f"\n===== Running: {cmd} =====")
    os.system(cmd)

# ----------------------------
# æ‰§è¡Œå®éªŒ
# ----------------------------
for model, cfg in gpu_models.items():
    bs = cfg["batch"]

    # æ¨ç†ï¼šçŸ­åºåˆ—
    run_cmd(
        f"python test_llm.py --mode infer --model_name {model} "
        f"--repeats 3 --seq_len 128 --batch_size {bs} "
        f"--dtype fp16 --log_file {LOG_FILE}"
    )

    # æ¨ç†ï¼šé•¿åºåˆ—
    run_cmd(
        f"python test_llm.py --mode infer --model_name {model} "
        f"--repeats 3 --seq_len 512 --batch_size {bs} "
        f"--dtype fp16 --log_file {LOG_FILE}"
    )

    # è®­ç»ƒï¼šçŸ­åºåˆ—
    run_cmd(
        f"python test_llm.py --mode train --model_name {model} "
        f"--seq_len 128 --batch_size {bs} --dtype fp16 "
        f"--log_file {LOG_FILE}"
    )

print("\nâœ… GPU å®éªŒå®Œæˆï¼Œç»“æœå†™å…¥ logs/test_llm.jsonl")

import os, json

LOG_FILE = "logs/test_llm.jsonl"
os.makedirs("logs", exist_ok=True)

# ----------------------------
# å…¨éƒ¨ç›®æ ‡ GPU æ¨¡å‹
# ----------------------------
gpu_models = {
    "facebook/opt-1.3b": {"batch": 2},
    "EleutherAI/pythia-1b": {"batch": 2},
    "bigscience/bloom-1b1": {"batch": 1},
    "mistralai/Mistral-7B-v0.1": {"batch": 1},
    "Qwen/Qwen-7B": {"batch": 1},
    "tiiuae/falcon-7b": {"batch": 1},
    "google/gemma-7b": {"batch": 1},  # å¦‚æœ HF æˆæƒå¯ç”¨
}

# ----------------------------
# è¯»å–ç°æœ‰æ—¥å¿—ï¼Œçœ‹çœ‹å“ªäº›æ¨¡å‹å·²ç»æœ‰ç»“æœ
# ----------------------------
existing = set()
if os.path.exists(LOG_FILE):
    with open(LOG_FILE, "r", encoding="utf-8") as f:
        for line in f:
            try:
                rec = json.loads(line)
                if isinstance(rec, dict) and "model" in rec:
                    existing.add(rec["model"])
            except:
                continue

print("ğŸ“Š æ—¥å¿—ä¸­å·²æœ‰æ¨¡å‹:", existing)

# ----------------------------
# æ‰§è¡Œå®éªŒï¼ˆåªè·‘ç¼ºå¤±çš„ï¼‰
# ----------------------------
def run_cmd(cmd):
    print(f"\n===== Running: {cmd} =====")
    os.system(cmd)

for model, cfg in gpu_models.items():
    if model in existing:
        print(f"â­ï¸ è·³è¿‡ {model}ï¼ˆå·²æœ‰ç»“æœï¼‰")
        continue

    bs = cfg["batch"]
    print(f"\nğŸš€ å¼€å§‹è¡¥è·‘ {model}")

    # æ¨ç†ï¼šçŸ­åºåˆ—
    run_cmd(
        f"python test_llm.py --mode infer --model_name {model} "
        f"--repeats 3 --seq_len 128 --batch_size {bs} "
        f"--dtype fp16 --log_file {LOG_FILE}"
    )

    # æ¨ç†ï¼šé•¿åºåˆ—
    run_cmd(
        f"python test_llm.py --mode infer --model_name {model} "
        f"--repeats 3 --seq_len 512 --batch_size {bs} "
        f"--dtype fp16 --log_file {LOG_FILE}"
    )

    # è®­ç»ƒï¼šçŸ­åºåˆ—
    run_cmd(
        f"python test_llm.py --mode train --model_name {model} "
        f"--seq_len 128 --batch_size {bs} --dtype fp16 "
        f"--log_file {LOG_FILE}"
    )

print("\nâœ… GPU è¡¥è·‘å®Œæˆï¼Œç»“æœå†™å…¥ logs/test_llm.jsonl")

import os, json
from huggingface_hub import login

# ----------------------------
# ç™»å½• Hugging Faceï¼ˆå¡«å…¥ä½ çš„ tokenï¼‰
# ----------------------------


LOG_FILE = "logs/test_llm.jsonl"
os.makedirs("logs", exist_ok=True)

gpu_models = {
    "facebook/opt-1.3b": {"batch": 2},
    "EleutherAI/pythia-1b": {"batch": 2},
    "bigscience/bloom-1b1": {"batch": 1},
    "mistralai/Mistral-7B-v0.1": {"batch": 1},
    "tiiuae/falcon-7b": {"batch": 1},
    "google/gemma-7b": {"batch": 1},
}

def run_cmd(cmd):
    print(f"\n===== Running: {cmd} =====")
    os.system(cmd)

for model, cfg in gpu_models.items():
    bs = cfg["batch"]

    run_cmd(f"python test_llm.py --mode infer --model_name {model} "
            f"--repeats 3 --seq_len 128 --batch_size {bs} "
            f"--log_file {LOG_FILE}")

    run_cmd(f"python test_llm.py --mode infer --model_name {model} "
            f"--repeats 3 --seq_len 512 --batch_size {bs} "
            f"--log_file {LOG_FILE}")

    run_cmd(f"python test_llm.py --mode train --model_name {model} "
            f"--seq_len 128 --batch_size {bs} "
            f"--log_file {LOG_FILE}")

print("\nâœ… GPU å®éªŒå®Œæˆï¼Œç»“æœå†™å…¥ logs/test_llm.jsonl")

import json
import pandas as pd

LOG_FILE = "logs/test_llm.jsonl"
LEADERBOARD_FIXED = "leaderboard_fixed.csv"

# 1. è¯» JSONL
rows = []
with open(LOG_FILE, "r", encoding="utf-8") as f:
    for line in f:
        try:
            rec = json.loads(line)
            if isinstance(rec, dict):
                rows.append(rec)
        except:
            continue

df = pd.DataFrame(rows)
print("âœ… è¯»å…¥æ—¥å¿—:", df.shape)

# 2. åŠ  device åˆ—
cpu_models = ["distilgpt2", "gpt2", "gpt2-medium", "gpt2-large", "gpt2-xl"]
df["device"] = df["model"].apply(lambda m: "CPU" if m in cpu_models else "GPU")

# 3. èšåˆï¼ˆfixed è¡¨ï¼‰â€”â€”åŒä¸€é…ç½®å–å¹³å‡
group_keys = ["model","task","device"]
agg_dict = {}
for c in df.columns:
    if c not in group_keys:
        if pd.api.types.is_numeric_dtype(df[c]):
            agg_dict[c] = "mean"
        else:
            agg_dict[c] = "first"

df_fixed = df.groupby(group_keys, dropna=False).agg(agg_dict).reset_index()
df_fixed["runs_count"] = df.groupby(group_keys).size().values

# 4. ä¿å­˜
df_fixed.to_csv(LEADERBOARD_FIXED, index=False)
print("âœ… å·²ç”Ÿæˆ", LEADERBOARD_FIXED, "å¤§å°:", df_fixed.shape)
display(df_fixed.head())

import pandas as pd
import matplotlib.pyplot as plt

# è¯»å–æ•°æ®
df = pd.read_csv("leaderboard_fixed.csv")

# æ¨¡å‹å‚æ•°è§„æ¨¡ (å•ä½: å‚æ•°ä¸ªæ•°)
param_counts = {
    "distilgpt2": 82e6,
    "gpt2": 124e6,
    "gpt2-medium": 355e6,
    "gpt2-large": 774e6,
    "gpt2-xl": 1.5e9,
    "facebook/opt-1.3b": 1.3e9,
    "EleutherAI/pythia-1b": 1e9,
    "bigscience/bloom-1b1": 1.1e9,
    "tiiuae/falcon-7b": 7e9,
    "mistralai/Mistral-7B-v0.1": 7e9,
    "google/gemma-7b": 7e9,
}
df["param_count"] = df["model"].map(param_counts)

# ç»˜åˆ¶ CPU vs GPU ååç‡
plt.figure(figsize=(8,6))
for device, group in df.groupby("device"):
    plt.plot(group["param_count"], group["tokens_per_s"], "o-", label=device)

plt.xscale("log")
plt.yscale("log")
plt.xlabel("Model size (parameters)")
plt.ylabel("Throughput (tokens/s)")
plt.title("Throughput vs Model Size (CPU vs GPU)")
plt.legend()
plt.grid(True, which="both", ls="--", alpha=0.6)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LinearRegression

df = pd.read_csv("leaderboard_fixed.csv")

# å‚æ•°è§„æ¨¡
param_counts = {
    "distilgpt2": 82e6,
    "gpt2": 124e6,
    "gpt2-medium": 355e6,
    "gpt2-large": 774e6,
    "gpt2-xl": 1.5e9,
    "facebook/opt-1.3b": 1.3e9,
    "EleutherAI/pythia-1b": 1e9,
    "bigscience/bloom-1b1": 1.1e9,
    "tiiuae/falcon-7b": 7e9,
    "mistralai/Mistral-7B-v0.1": 7e9,
    "google/gemma-7b": 7e9,
}
df["param_count"] = df["model"].map(param_counts)

# -------------------
# å›¾ 1: æ¨¡å‹è§„æ¨¡ vs ç¢³æ’æ”¾
# -------------------
plt.figure(figsize=(8,6))
for device, group in df.groupby("device"):
    plt.plot(group["param_count"], group["emissions"], "o-", label=device)
plt.xscale("log")
plt.yscale("log")
plt.xlabel("Model size (parameters)")
plt.ylabel("Emissions (kg COâ‚‚e)")
plt.title("Model Size vs Emissions (CPU vs GPU)")
plt.legend()
plt.grid(True, which="both", ls="--", alpha=0.6)
plt.show()

# -------------------
# å›¾ 2: èƒ½æ•ˆ vs æ¨¡å‹è§„æ¨¡
# -------------------
df["efficiency"] = df["emissions"] / df["flops"]

plt.figure(figsize=(8,6))
for device, group in df.groupby("device"):
    plt.plot(group["param_count"], group["efficiency"], "o-", label=device)
plt.xscale("log")
plt.yscale("log")
plt.xlabel("Model size (parameters)")
plt.ylabel("Emissions per FLOP (kg COâ‚‚e / FLOP)")
plt.title("Energy Efficiency vs Model Size")
plt.legend()
plt.grid(True, which="both", ls="--", alpha=0.6)
plt.show()

# -------------------
# å›¾ 3: FLOPs vs ç¢³æ’æ”¾ (log-log å›å½’)
# -------------------
plt.figure(figsize=(8,6))
for device, group in df.groupby("device"):
    plt.scatter(group["flops"], group["emissions"], alpha=0.7, label=device)

# å›å½’æ‹Ÿåˆ (log-log)
X = np.log10(df["flops"].values.reshape(-1,1))
y = np.log10(df["emissions"].values)
reg = LinearRegression().fit(X, y)
x_fit = np.linspace(X.min(), X.max(), 100).reshape(-1,1)
y_fit = reg.predict(x_fit)
plt.plot(10**x_fit.flatten(), 10**y_fit, "k--", label=f"Trend slope={reg.coef_[0]:.2f}")

plt.xscale("log")
plt.yscale("log")
plt.xlabel("FLOPs")
plt.ylabel("Emissions (kg COâ‚‚e)")
plt.title("FLOPs vs Emissions (log-log)")
plt.legend()
plt.grid(True, which="both", ls="--", alpha=0.6)
plt.show()

import pandas as pd
df = pd.read_csv("leaderboard_fixed.csv")
print(df.columns)

import pandas as pd
df = pd.read_csv("leaderboard_fixed.csv")
print(df["model"].unique())
print(len(df))

import pandas as pd

df = pd.read_csv("leaderboard_fixed.csv")
print(df.columns)

import pandas as pd

# è¯»å– fixed ç‰ˆæœ¬çš„ leaderboard
df_fixed = pd.read_csv("leaderboard_fixed.csv")

print("âœ… è¯»å–æˆåŠŸï¼Œè¡¨æ ¼å¤§å°:", df_fixed.shape)
display(df_fixed.head())

import pandas as pd
import json
import os

# ====== æ–‡ä»¶è·¯å¾„é…ç½® ======
LOG_JSONL = "logs/test_llm.jsonl"         # æ—¥å¿—æ–‡ä»¶
LEADERBOARD_FIXED = "leaderboard_fixed.csv"  # å·²æœ‰ fixed è¡¨ï¼ˆä¸åœ¨ logs ä¸‹ï¼‰

# ====== 1. è¯»å·²æœ‰ fixed è¡¨ ======
if os.path.exists(LEADERBOARD_FIXED):
    df_fixed_old = pd.read_csv(LEADERBOARD_FIXED)
    print("âœ… è¯»å–åŸæ¥çš„ fixed è¡¨:", df_fixed_old.shape, "åˆ—:", df_fixed_old.columns.tolist())
else:
    df_fixed_old = pd.DataFrame()
    print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ—§çš„ leaderboard_fixed.csvï¼Œå°†åªç”¨ log ç”Ÿæˆæ–°çš„ã€‚")

# ====== 2. ä» jsonl è¯»å–æ–°ç»“æœ ======
rows = []
if os.path.exists(LOG_JSONL):
    with open(LOG_JSONL, "r", encoding="utf-8") as f:
        for line in f:
            try:
                rec = json.loads(line)
                if rec.get("_record_type") == "result-summary":
                    rows.append(rec)
            except:
                continue
    df_new = pd.DataFrame(rows)
    print("âœ… ä»æ—¥å¿—è¯»åˆ°æ–°ç»“æœ:", df_new.shape, "åˆ—:", df_new.columns.tolist())
else:
    df_new = pd.DataFrame()
    print("âš ï¸ æ²¡æœ‰æ‰¾åˆ° logs/test_llm.jsonl")

# ====== 3. åˆå¹¶è€çš„ fixed è¡¨å’Œæ–°æ•°æ® ======
if not df_fixed_old.empty and not df_new.empty:
    df_all = pd.concat([df_fixed_old, df_new], ignore_index=True)
elif not df_fixed_old.empty:
    df_all = df_fixed_old.copy()
else:
    df_all = df_new.copy()

print("ğŸ“Š åˆå¹¶åæ•°æ®å¤§å°:", df_all.shape)

# ====== 4. è‡ªåŠ¨é€‰æ‹©åˆ†ç»„é”® ======
possible_keys = ["model","mode","seq_len","batch_size","dtype","task"]
group_keys = [k for k in possible_keys if k in df_all.columns]
print("ğŸ”‘ ä½¿ç”¨çš„åˆ†ç»„é”®:", group_keys)

# ====== 5. èšåˆï¼šæ•°å€¼å–å¹³å‡ï¼Œå­—ç¬¦ä¸²å–ç¬¬ä¸€ä¸ª ======
agg_dict = {}
for c in df_all.columns:
    if c not in group_keys:
        if pd.api.types.is_numeric_dtype(df_all[c]):
            agg_dict[c] = "mean"
        else:
            agg_dict[c] = "first"

df_fixed_new = df_all.groupby(group_keys, dropna=False).agg(agg_dict).reset_index()
df_fixed_new["runs_count"] = df_all.groupby(group_keys).size().values

# ====== 6. ä¿å­˜ ======
df_fixed_new.to_csv(LEADERBOARD_FIXED, index=False)
print("âœ… å·²æ›´æ–°", LEADERBOARD_FIXED, "æ–°è¡¨å¤§å°:", df_fixed_new.shape)
display(df_fixed_new.head())

import pandas as pd
import json
import os

LOG_JSONL = "logs/test_llm.jsonl"
LEADERBOARD_FIXED = "leaderboard_fixed.csv"

# 1. è¯»å–æ—¥å¿—
rows = []
with open(LOG_JSONL, "r", encoding="utf-8") as f:
    for line in f:
        try:
            rec = json.loads(line)
            if rec.get("_record_type") == "result-summary":
                rows.append(rec)
        except:
            continue

df = pd.DataFrame(rows)
print("âœ… è¯»å…¥æ—¥å¿—:", df.shape, "åˆ—:", df.columns.tolist())

# 2. ç¡®å®šåˆ†ç»„é”®ï¼ˆæ ¹æ®æ—¥å¿—é‡Œå®é™…å­—æ®µè‡ªåŠ¨æŒ‘é€‰ï¼‰
possible_keys = ["model","mode","seq_len","batch_size","dtype","task"]
group_keys = [k for k in possible_keys if k in df.columns]
print("ğŸ”‘ ä½¿ç”¨åˆ†ç»„é”®:", group_keys)

# 3. èšåˆé€»è¾‘ï¼šæ•°å€¼åˆ—å–å¹³å‡ï¼Œå­—ç¬¦ä¸²åˆ—å–ç¬¬ä¸€ä¸ª
agg_dict = {}
for c in df.columns:
    if c not in group_keys:
        if pd.api.types.is_numeric_dtype(df[c]):
            agg_dict[c] = "mean"
        else:
            agg_dict[c] = "first"

df_fixed = df.groupby(group_keys, dropna=False).agg(agg_dict).reset_index()
df_fixed["runs_count"] = df.groupby(group_keys).size().values

# 4. ä¿å­˜ä¸ºæ–°çš„ leaderboard_fixed.csv
df_fixed.to_csv(LEADERBOARD_FIXED, index=False)
print("âœ… å·²ç”Ÿæˆæ–°çš„", LEADERBOARD_FIXED, "å¤§å°:", df_fixed.shape)
display(df_fixed.head())

import pandas as pd
import json

LOG_JSONL = "logs/test_llm.jsonl"
LEADERBOARD_FIXED = "leaderboard_fixed.csv"

# 1. è¯» JSONL
rows = []
with open(LOG_JSONL, "r", encoding="utf-8") as f:
    for line in f:
        try:
            rec = json.loads(line)
            if isinstance(rec, dict):
                rows.append(rec)
        except:
            continue

df = pd.DataFrame(rows)
print("âœ… è¯»å…¥æ—¥å¿—:", df.shape, "åˆ—:", df.columns.tolist())

# 2. ç¡®å®šåˆ†ç»„é”®
possible_keys = ["model","task","mode","seq_len","batch_size","dtype"]
group_keys = [k for k in possible_keys if k in df.columns]
print("ğŸ”‘ ä½¿ç”¨åˆ†ç»„é”®:", group_keys)

# 3. èšåˆé€»è¾‘ï¼šæ•°å€¼åˆ—å–å¹³å‡ï¼Œå­—ç¬¦ä¸²åˆ—å–ç¬¬ä¸€ä¸ª
agg_dict = {}
for c in df.columns:
    if c not in group_keys:
        if pd.api.types.is_numeric_dtype(df[c]):
            agg_dict[c] = "mean"
        else:
            agg_dict[c] = "first"

df_fixed = df.groupby(group_keys, dropna=False).agg(agg_dict).reset_index()
df_fixed["runs_count"] = df.groupby(group_keys).size().values

# 4. ä¿å­˜
df_fixed.to_csv(LEADERBOARD_FIXED, index=False)
print("âœ… å·²ç”Ÿæˆ", LEADERBOARD_FIXED, "å¤§å°:", df_fixed.shape)
display(df_fixed.head())

import pandas as pd
import matplotlib.pyplot as plt

# è¯»å–ä¿®å¤åçš„ leaderboard
df = pd.read_csv("leaderboard_fixed.csv")

# ç»™æ¨¡å‹åŠ ä¸Šå‚æ•°é‡ï¼ˆè¿‘ä¼¼å€¼ï¼Œæ–¹ä¾¿ç”» scaling lawï¼‰
param_counts = {
    "distilgpt2": 82e6,
    "gpt2": 124e6,
    "gpt2-medium": 355e6,
    "gpt2-large": 774e6,
    "gpt2-xl": 1.5e9,
    "tiiuae/falcon-7b": 7e9,
}
df["params"] = df["model"].map(param_counts)

# å‚æ•°é‡ vs èƒ½è€—
plt.figure(figsize=(8,5))
for task in df["task"].unique():
    subset = df[df["task"] == task]
    plt.scatter(subset["params"], subset["emissions"], label=task, alpha=0.7)

plt.xscale("log")
plt.yscale("log")
plt.xlabel("Model Parameters (log scale)")
plt.ylabel("Energy Consumption (kWh, log scale)")
plt.title("Scaling Law: Params vs Energy")
plt.legend()
plt.show()

# FLOPs vs èƒ½è€—
plt.figure(figsize=(8,5))
plt.scatter(df["flops"], df["emissions"], c="red", alpha=0.7)
plt.xscale("log")
plt.yscale("log")
plt.xlabel("FLOPs (log scale)")
plt.ylabel("Energy Consumption (kWh, log scale)")
plt.title("FLOPs vs Energy Consumption")
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# è¯»å–ä¿®å¤åçš„ leaderboard
df = pd.read_csv("leaderboard_fixed.csv")

# æ¨¡å‹å‚æ•°é‡æ˜ å°„
param_counts = {
    "distilgpt2": 82e6,
    "gpt2": 124e6,
    "gpt2-medium": 355e6,
    "gpt2-large": 774e6,
    "gpt2-xl": 1.5e9,
    "tiiuae/falcon-7b": 7e9,
}
df["params"] = df["model"].map(param_counts)

plt.figure(figsize=(8,6))

# ===== GPT-2 ç³»åˆ— (æ¨ç†æ›²çº¿) =====
gpt2_models = ["distilgpt2", "gpt2", "gpt2-medium", "gpt2-large", "gpt2-xl"]

for task, color in zip(["inference-short", "inference-long"], ["#1f77b4", "#ff7f0e"]):
    subset = df[(df["task"] == task) & (df["model"].isin(gpt2_models))]
    subset = subset.sort_values("params")
    plt.plot(subset["params"], subset["emissions"], marker="o", color=color, linewidth=2.5, label=f"GPT-2 {task}")

# ===== Falcon-7B (å¯¹ç…§ç‚¹) =====
for task, color, marker in zip(["inference-short", "inference-long"], ["#1f77b4", "#ff7f0e"], ["^", "v"]):
    subset = df[(df["task"] == task) & (df["model"] == "tiiuae/falcon-7b")]
    plt.scatter(subset["params"], subset["emissions"], marker=marker, s=150, color=color, edgecolor="black", zorder=5)
    for _, row in subset.iterrows():
        plt.text(row["params"]*1.05, row["emissions"], "Falcon-7B", fontsize=10, weight="bold", color=color)

# ===== è®­ç»ƒä»»åŠ¡ (èƒŒæ™¯æ•£ç‚¹) =====
train_subset = df[df["task"].str.contains("training")]
plt.scatter(train_subset["params"], train_subset["emissions"], alpha=0.4, color="gray", label="Training tasks")

# ===== åæ ‡ & æ ·å¼ =====
plt.xscale("log")
plt.yscale("log")
plt.xlabel("Model Parameters", fontsize=12)
plt.ylabel("Energy Consumption (kWh)", fontsize=12)
plt.title("Scaling Law: GPT-2 vs Falcon-7B", fontsize=14, weight="bold")

# å›¾ä¾‹åªä¿ç•™ä¸‰ç±»
handles, labels = plt.gca().get_legend_handles_labels()
by_label = dict(zip(labels, handles))
plt.legend(by_label.values(), by_label.keys(), loc="upper left", frameon=False)

plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

df = pd.read_csv("leaderboard_fixed.csv")

plt.figure(figsize=(8,6))

for task in ["inference-short", "inference-long", "training-b2-s128"]:
    subset = df[df["task"] == task]
    x = subset["flops"].values
    y = subset["emissions"].values
    plt.scatter(x, y, label=f"{task} data")

    # log-log å›å½’
    log_x = np.log10(x)
    log_y = np.log10(y)
    coef = np.polyfit(log_x, log_y, 1)
    trend_y = np.polyval(coef, log_x)
    order = np.argsort(x)
    plt.plot(x[order], 10**trend_y[order], linewidth=2, label=f"{task} fit (slope={coef[0]:.2f})")

plt.xscale("log")
plt.yscale("log")
plt.xlabel("FLOPs")
plt.ylabel("Energy Consumption (kWh)")
plt.title("FLOPs vs Energy Consumption (Grouped Fits)")
plt.legend()
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

df = pd.read_csv("leaderboard_fixed.csv")

# åªä¿ç•™å¤§æ¨¡å‹ (â‰¥ gpt2-medium)
large_models = ["gpt2-medium", "gpt2-large", "gpt2-xl", "tiiuae/falcon-7b"]
df_large = df[df["model"].isin(large_models)]

plt.figure(figsize=(8,6))

# ç»˜åˆ¶å„ä»»åŠ¡æ‹Ÿåˆ + æ•°æ®ç‚¹
colors = {"inference-short": "blue", "inference-long": "orange", "training-b2-s128": "green"}

for task in ["inference-short", "inference-long", "training-b2-s128"]:
    subset = df_large[df_large["task"] == task]
    x = subset["flops"].values
    y = subset["emissions"].values

    if len(x) < 3:
        continue

    plt.scatter(x, y, label=f"{task} data", color=colors[task], alpha=0.7)

    # log-log å›å½’
    log_x = np.log10(x)
    log_y = np.log10(y)
    coef = np.polyfit(log_x, log_y, 1)
    trend_y = np.polyval(coef, log_x)
    order = np.argsort(x)
    plt.plot(x[order], 10**trend_y[order], color=colors[task], linewidth=2,
             label=f"{task} fit (slope={coef[0]:.2f})")

# ===== slope=1 ç†æƒ³å‚è€ƒçº¿ =====
x_ref = np.logspace(9, 14, 100)
y_ref = 1e-4 * (x_ref / x_ref[0])  # èµ·ç‚¹å½’ä¸€åŒ–
plt.plot(x_ref, y_ref, "--", color="gray", linewidth=2, label="Ideal slope=1")

# åæ ‡è½´ & æ ·å¼
plt.xscale("log")
plt.yscale("log")
plt.xlabel("FLOPs", fontsize=12)
plt.ylabel("Energy Consumption (kWh)", fontsize=12)
plt.title("FLOPs vs Energy Consumption (Grouped Fits + Ideal Line)", fontsize=14, weight="bold")
plt.legend()
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# è¯»å–æ•°æ®
df = pd.read_csv("leaderboard_fixed.csv")

# ç­›é€‰ inference-short ä»»åŠ¡
subset = df[df["task"] == "inference-short"].copy()

# æ¯ä¸ª model + device å–å¹³å‡èƒ½è€—
subset = subset.groupby(["model", "device"])["emissions"].mean().reset_index()

# ç”»æŸ±çŠ¶å›¾
plt.figure(figsize=(8,6))
colors = {"CPU": "red", "GPU": "blue"}

for device in ["CPU", "GPU"]:
    dev_data = subset[subset["device"] == device]
    plt.bar(dev_data["model"], dev_data["emissions"],
            color=colors[device], label=device)

plt.xticks(rotation=30, ha="right")
plt.yscale("log")
plt.ylabel("Energy Consumption (kWh)")
plt.title("CPU vs GPU Energy Comparison (Inference-Short)")
plt.legend()
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# è¯»å–æ•°æ®
df = pd.read_csv("leaderboard_fixed.csv")

# æ¨¡å‹å‚æ•°è§„æ¨¡
param_counts = {
    "distilgpt2": 82e6,
    "gpt2": 124e6,
    "gpt2-medium": 355e6,
    "gpt2-large": 774e6,
    "gpt2-xl": 1.5e9,
    "facebook/opt-1.3b": 1.3e9,
    "EleutherAI/pythia-1b": 1e9,
    "bigscience/bloom-1b1": 1.1e9,
    "tiiuae/falcon-7b": 7e9,
    "mistralai/Mistral-7B-v0.1": 7e9,
    "google/gemma-7b": 7e9,
}
df["param_count"] = df["model"].map(param_counts)

# ç”»å›¾ï¼šæ¯ä¸ª task ä¸€æ¡æ›²çº¿
plt.figure(figsize=(8,6))
for task, group in df.groupby("task"):
    plt.scatter(group["param_count"], group["emissions"], label=task, alpha=0.8)

plt.xscale("log")
plt.yscale("log")
plt.xlabel("Model Parameters (log scale)")
plt.ylabel("Energy Consumption (kWh, log scale)")
plt.title("Scaling Law: Params vs Energy (Task-level)")
plt.legend()
plt.grid(True, which="both", ls="--", alpha=0.6)
plt.show()

import pandas as pd

# è¯»å– summary.csv
df = pd.read_csv("summary.csv")

# ====== 1. å®šä¹‰æ¨¡å‹è§„æ¨¡ (å‚æ•°é‡) ======
param_map = {
    "distilgpt2": "82M",
    "gpt2": "124M",
    "gpt2-medium": "355M",
    "gpt2-large": "774M",
    "gpt2-xl": "1.5B",
    "EleutherAI/pythia-1b": "1.0B",
    "facebook/opt-1.3b": "1.3B",
    "bigscience/bloom-1b1": "1.1B",
    "google/gemma-7b": "7B",
    "mistralai/Mistral-7B-v0.1": "7B",
    "tiiuae/falcon-7b": "7B"
}

# ====== 2. å¢åŠ  Params åˆ— ======
df["params"] = df["model"].map(param_map)

# ====== 3. è°ƒæ•´åˆ—é¡ºåº ======
df = df[["model", "params", "device", "flops", "emissions", "efficiency", "runs"]]

# ä¿å­˜æ–°çš„ CSV
df.to_csv("summary_with_params.csv", index=False)

# ====== 4. è¾“å‡º LaTeX è¡¨æ ¼ ======
latex_table = df.to_latex(
    index=False,
    float_format="%.2e",
    caption="Summary of benchmark results across models, parameters, and devices.",
    label="tab:summary"
)

with open("summary_with_params.tex", "w") as f:
    f.write(latex_table)

print("âœ… å·²ç”Ÿæˆ summary_with_params.csv å’Œ summary_with_params.tex")
print("\nğŸ“‘ LaTeX é¢„è§ˆ:\n")
print(latex_table)

import os
import json
import pandas as pd
from codecarbon import EmissionsTracker

# ====== 1. è·‘ CodeCarbon ======
def run_codecarbon(model_name, seq_len=128, repeats=3):
    log_file = f"logs/codecarbon_{model_name}_s{seq_len}.json"
    tracker = EmissionsTracker(output_file=log_file)
    tracker.start()

    # è°ƒç”¨ä½ çš„ benchmark è„šæœ¬ (åªè·‘ inference)
    os.system(f"python test_llm.py --mode infer --model_name {model_name} "
              f"--seq_len {seq_len} --repeats {repeats} --log_file {log_file}")

    emissions = tracker.stop()
    print(f"Model {model_name}, seq_len={seq_len} â†’ {emissions:.4e} kgCOâ‚‚e")
    return emissions

# ç¤ºä¾‹ï¼šè·‘ GPT-2 å’Œ Falcon-7B
models_to_test = [
    ("gpt2", 128),
    ("gpt2", 512),
    ("tiiuae/falcon-7b", 128),
    ("tiiuae/falcon-7b", 512),
]

results_cc = []
for model, seq in models_to_test:
    e = run_codecarbon(model, seq_len=seq, repeats=3)
    results_cc.append({"model": model, "task": f"inference-s{seq}", "device": "GPU", "CodeCarbon": e})

df_cc = pd.DataFrame(results_cc)

# ====== 2. è¯»å–ä½ è‡ªå·±çš„ benchmark ======
df_bench = pd.read_csv("leaderboard_fixed.csv")

# åªä¿ç•™ inference-GPU éƒ¨åˆ†
df_bench = df_bench[df_bench["device"]=="GPU"]

# ====== 3. åŒ¹é…æ¨¡å‹+ä»»åŠ¡ï¼Œç”Ÿæˆå¯¹æ¯”è¡¨ ======
df_compare = pd.merge(df_cc, df_bench, on=["model","task","device"], how="inner")

# é‡å‘½ååˆ—
df_compare = df_compare.rename(columns={"emissions": "OurBenchmark"})

# è®¡ç®—å·®å¼‚
df_compare["Delta_%"] = (df_compare["OurBenchmark"] - df_compare["CodeCarbon"]) / df_compare["CodeCarbon"] * 100

# ====== 4. ä¿å­˜ CSV + LaTeX ======
df_compare.to_csv("compare_codecarbon.csv", index=False)

latex_table = df_compare[["model","task","device","CodeCarbon","OurBenchmark","Delta_%"]].to_latex(
    index=False,
    float_format="%.2e",
    caption="Comparison of CodeCarbon vs Our Benchmark (GPU Inference).",
    label="tab:compare_codecarbon"
)
with open("compare_codecarbon.tex", "w") as f:
    f.write(latex_table)

print("âœ… å·²ç”Ÿæˆ compare_codecarbon.csv å’Œ compare_codecarbon.tex")
print(df_compare)

import os
import pandas as pd
from codecarbon import EmissionsTracker

# ç¡®ä¿ logs ç›®å½•å­˜åœ¨
LOG_FILE = "logs/test_llm.jsonl"
os.makedirs("logs", exist_ok=True)

def run_codecarbon(model_name, mode="infer", seq_len=128, batch_size=1, repeats=3):
    """
    ç”¨ CodeCarbon è·‘ä¸€æ¬¡å®éªŒï¼Œè¿”å› emissions
    """
    tracker = EmissionsTracker(
        output_dir="logs",
        output_file="cc_emissions.csv"   # âœ… æ”¹æˆå›ºå®šæ–‡ä»¶ï¼Œä¸è¦ None
    )
    tracker.start()

    # æ„å»ºå‘½ä»¤
    if mode == "infer":
        cmd = (
            f"python test_llm.py --mode infer --model_name {model_name} "
            f"--seq_len {seq_len} --repeats {repeats} --batch_size {batch_size} "
            f"--log_file {LOG_FILE}"
        )
    else:
        cmd = (
            f"python test_llm.py --mode train --model_name {model_name} "
            f"--seq_len {seq_len} --batch_size {batch_size} "
            f"--log_file {LOG_FILE}"
        )

    print(f"\n===== Running with CodeCarbon: {cmd} =====")
    os.system(cmd)

    emissions = tracker.stop()
    if emissions is None:
        emissions = 0.0

    print(f"[CodeCarbon] {model_name}, {mode}, seq={seq_len}, batch={batch_size} â†’ {emissions:.4e} kgCOâ‚‚e")
    return emissions


# -----------------------------
# è¦æµ‹è¯•çš„æ¨¡å‹ & é…ç½®
# -----------------------------
tasks_to_test = [
    ("gpt2", "infer", 128, 1),
    ("gpt2", "train", 128, 1),
    ("gpt2-medium", "infer", 128, 1),
    ("tiiuae/falcon-7b", "infer", 128, 1),
]

# -----------------------------
# å¾ªç¯è·‘å®éªŒ & ä¿å­˜ç»“æœ
# -----------------------------
results_cc = []

for model, mode, seq, batch in tasks_to_test:
    e = run_codecarbon(model, mode=mode, seq_len=seq, batch_size=batch, repeats=3)
    results_cc.append({
        "model": model,
        "mode": mode,
        "seq_len": seq,
        "batch_size": batch,
        "device": "GPU",
        "CodeCarbon_emissions": e
    })

# ä¿å­˜åˆ° DataFrame
df_cc = pd.DataFrame(results_cc)
df_cc.to_csv("results_codecarbon.csv", index=False)

print("\nâœ… CodeCarbon å®éªŒå®Œæˆï¼Œç»“æœå†™å…¥ results_codecarbon.csv")
display(df_cc)

import pandas as pd

# 1. è¯»å–ä¸¤ä¸ªæ–‡ä»¶
df_my = pd.read_csv("leaderboard_fixed.csv")
df_cc = pd.read_csv("results_codecarbon.csv")

# 2. è½¬æ¢ CodeCarbon çš„ task åç§°ä¸ºç»Ÿä¸€æ ¼å¼
def format_task(row):
    if row["mode"] == "infer":
        return f"inference-s{row['seq_len']}"
    else:
        return f"train-s{row['seq_len']}-b{row['batch_size']}"

df_cc["task"] = df_cc.apply(format_task, axis=1)

# 3. ä¿ç•™éœ€è¦çš„åˆ—
df_my_small = df_my[["model","task","device","emissions"]].rename(columns={"emissions":"My_emissions"})
df_cc_small = df_cc[["model","task","device","CodeCarbon_emissions"]]

# 4. åˆå¹¶
df_compare = pd.merge(df_my_small, df_cc_small, on=["model","task","device"], how="inner")

# 5. è®¡ç®—å·®å¼‚
df_compare["Diff_%"] = (df_compare["My_emissions"] - df_compare["CodeCarbon_emissions"]) / df_compare["CodeCarbon_emissions"] * 100

# 6. ä¿å­˜ CSV
df_compare.to_csv("compare_my_vs_codecarbon.csv", index=False)

# 7. ç”Ÿæˆ LaTeX è¡¨æ ¼
latex_table = df_compare.to_latex(
    index=False,
    float_format="%.2e",
    caption="Comparison of CodeCarbon vs Our Benchmark",
    label="tab:compare_codecarbon"
)
with open("compare_my_vs_codecarbon.tex","w") as f:
    f.write(latex_table)

print("âœ… å·²ç”Ÿæˆ compare_my_vs_codecarbon.csv å’Œ compare_my_vs_codecarbon.tex")
print(df_compare)

import os
import pandas as pd
from codecarbon import EmissionsTracker

# 1. è¯»å– leaderboard
df = pd.read_csv("leaderboard_fixed.csv")
df_gpu = df[df["device"].str.upper() == "GPU"].copy()

# 2. è§£æ task â†’ (mode, seq_len, batch_size)
def parse_task(task):
    if task.startswith("inference"):
        mode = "infer"
        if "short" in task:
            seq_len = 128
        elif "long" in task:
            seq_len = 512
        else:
            seq_len = 128
        batch_size = 1
    elif task.startswith("training"):
        mode = "train"
        parts = task.split("-")
        b = [p for p in parts if p.startswith("b")]
        s = [p for p in parts if p.startswith("s")]
        batch_size = int(b[0][1:]) if b else 1
        seq_len = int(s[0][1:]) if s else 128
    else:
        mode, seq_len, batch_size = "infer", 128, 1
    return mode, seq_len, batch_size

df_gpu[["mode", "seq_len", "batch_size"]] = df_gpu.apply(
    lambda row: parse_task(row["task"]), axis=1, result_type="expand"
)

# 3. CodeCarbon runner
def run_codecarbon(model, mode, seq_len, batch_size, repeats=1):
    tracker = EmissionsTracker(output_dir=".", output_file="cc_tmp.csv", measure_power_secs=1)
    tracker.start()
    cmd = (
        f"python test_llm.py --mode {mode} "
        f"--model_name {model} --seq_len {seq_len} "
        f"--batch_size {batch_size} --repeats {repeats} --log_file cc_tmp.jsonl"
    )
    print(f"\n===== Running: {cmd} =====")
    os.system(cmd)
    emissions = tracker.stop()
    return emissions

# 4. éå† GPU å®éªŒç»„åˆ
results = []
for _, row in df_gpu.iterrows():
    model = row["model"]
    mode, seq_len, batch_size = row["mode"], row["seq_len"], row["batch_size"]
    e = run_codecarbon(model, mode, seq_len, batch_size, repeats=3)
    results.append({
        "model": model,
        "task": row["task"],
        "device": "GPU",
        "seq_len": seq_len,
        "batch_size": batch_size,
        "emissions_codecarbon": e
    })

# 5. ä¿å­˜ç»“æœ
df_cc = pd.DataFrame(results)
df_cc.to_csv("results_codecarbon_gpu.csv", index=False)
print("âœ… CodeCarbon GPU ç»“æœå·²ä¿å­˜åˆ° results_codecarbon_gpu.csv")
display(df_cc.head())

import pandas as pd

# 1. è¯»å–ä¸¤ä¸ªè¡¨
df_my = pd.read_csv("leaderboard_fixed.csv")
df_cc = pd.read_csv("results_codecarbon_gpu.csv")

# 2. ç»Ÿä¸€å¤§å°å†™
for col in ["model", "task", "device"]:
    df_my[col] = df_my[col].astype(str).str.strip().str.lower()
    df_cc[col] = df_cc[col].astype(str).str.strip().str.lower()

# 3. ä¿®å¤ task ä¸ä¸€è‡´çš„é—®é¢˜
# ä»¥ benchmark (df_my) çš„ task ä¸ºæ ‡å‡†
valid_tasks = set(df_my["task"].unique())

# å¦‚æœ CodeCarbon çš„ task ä¸åœ¨ benchmark é‡Œï¼Œå°è¯•æ˜ å°„
task_map = {
    "training-b1-s128": "training-b2-s128",  # âš ï¸ ä½ éœ€è¦ç¡®è®¤ benchmark æ˜¯ b2
    # å…¶ä»–ä¸ä¸€è‡´å¯ä»¥åœ¨è¿™é‡ŒåŠ 
}

df_cc["task"] = df_cc["task"].replace(task_map)

# 4. é€‰å–å¿…è¦å­—æ®µ
col_my = [c for c in df_my.columns if "emission" in c.lower()][0]
col_cc = [c for c in df_cc.columns if "emission" in c.lower()][0]

df_my_sel = df_my[["model", "task", "device", col_my]].rename(columns={col_my: "emissions_mine"})
df_cc_sel = df_cc[["model", "task", "device", col_cc]].rename(columns={col_cc: "emissions_codecarbon"})

# 5. åˆå¹¶ (inner = åªå¯¹æ¯”é‡å )
df_compare = pd.merge(df_my_sel, df_cc_sel, on=["model", "task", "device"], how="inner")

# 6. å·®å¼‚è®¡ç®—
df_compare["diff"] = df_compare["emissions_mine"] - df_compare["emissions_codecarbon"]
df_compare["diff_pct"] = df_compare["diff"] / df_compare["emissions_codecarbon"] * 100

# 7. ä¿å­˜ç»“æœ
df_compare.to_csv("compare_my_vs_codecarbon.csv", index=False)
with open("compare_my_vs_codecarbon.tex", "w") as f:
    f.write(df_compare.to_latex(index=False, float_format="%.4e"))

print("âœ… å¯¹æ¯”è¡¨å·²ç”Ÿæˆ: compare_my_vs_codecarbon.csv / .tex")
display(df_compare.head(20))

import pandas as pd

# è¯»ä¸¤ä¸ªè¡¨
df_my = pd.read_csv("leaderboard_fixed.csv")
df_cc = pd.read_csv("results_codecarbon.csv")

# ---- 1. æ ‡å‡†åŒ– emissions åˆ— ----
def find_emissions_col(df):
    for c in df.columns:
        if "emission" in c.lower():   # è‡ªåŠ¨æ‰¾åŒ…å« emission çš„åˆ—
            return c
    raise ValueError("æ²¡æœ‰æ‰¾åˆ° emissions åˆ—")

col_my = find_emissions_col(df_my)
col_cc = find_emissions_col(df_cc)

df_my_sel = df_my[["model", "task", "device", col_my]].rename(columns={col_my: "emissions_mine"})
df_cc_sel = df_cc[["model", "task", "device", col_cc]].rename(columns={col_cc: "emissions_codecarbon"})

# ---- 2. åˆå¹¶ (åªä¿ç•™é‡å éƒ¨åˆ†) ----
df_compare = pd.merge(
    df_my_sel,
    df_cc_sel,
    on=["model", "task", "device"],
    how="inner"   # æ”¹æˆ inner
)

# ---- 3. è®¡ç®—å·®å¼‚ ----
df_compare["diff"] = df_compare["emissions_mine"] - df_compare["emissions_codecarbon"]
df_compare["diff_pct"] = df_compare["diff"] / df_compare["emissions_codecarbon"] * 100

# ---- 4. ä¿å­˜ç»“æœ ----
df_compare.to_csv("compare_my_vs_codecarbon.csv", index=False)
with open("compare_my_vs_codecarbon.tex", "w") as f:
    f.write(df_compare.to_latex(index=False, float_format="%.4e"))

print("âœ… å¯¹æ¯”è¡¨ç”Ÿæˆå®Œæˆ (åªå«é‡å éƒ¨åˆ†): compare_my_vs_codecarbon.csv / .tex")
display(df_compare.head(20))

keys_my = set(zip(df_my_sel["model"], df_my_sel["task"], df_my_sel["device"]))
keys_cc = set(zip(df_cc_sel["model"], df_cc_sel["task"], df_cc_sel["device"]))

print("æˆ‘çš„è¡¨ key æ ·ä¾‹:", list(keys_my)[:10])
print("CodeCarbon key æ ·ä¾‹:", list(keys_cc)[:10])

print("é‡å æ•°é‡:", len(keys_my & keys_cc))
print("æˆ‘çš„æœ‰ä½† CodeCarbon æ²¡æœ‰:", len(keys_my - keys_cc))
print("CodeCarbon æœ‰ä½†æˆ‘çš„æ²¡æœ‰:", len(keys_cc - keys_my))

import pandas as pd

df_compare = pd.read_csv("compare_my_vs_codecarbon.csv")

# è½¬æ¢ä¸º LaTeX
latex_table = df_compare.to_latex(
    index=False,
    float_format="%.2e",
    caption="Comparison of emissions: Our method vs CodeCarbon",
    label="tab:compare_emissions"
)

with open("compare_my_vs_codecarbon.tex", "w") as f:
    f.write(latex_table)

print("âœ… å·²ç”Ÿæˆ LaTeX è¡¨æ ¼ â†’ compare_my_vs_codecarbon.tex")
print(latex_table)

# Scaling Law
plt.figure(figsize=(8,6))
for task in df["task"].unique():
    subset = df[df["task"] == task]
    plt.scatter(subset["params"], subset["emissions"], label=task, alpha=0.7)
plt.xscale("log"); plt.yscale("log")
plt.xlabel("Model Parameters")
plt.ylabel("Energy Consumption (kWh)")
plt.title("Scaling Law: Params vs Energy", fontsize=14, fontweight="normal")
plt.legend()
plt.tight_layout()
plt.savefig("scaling_law.png", dpi=300)

# FLOPs vs Energy (Grouped Fits)
plt.figure(figsize=(8,6))
for task in ["inference-short", "inference-long", "training-b2-s128"]:
    subset = df[df["task"] == task]
    if len(subset) > 2:
        plt.scatter(subset["flops"], subset["emissions"], alpha=0.6, label=f"{task} data")
        log_x, log_y = np.log10(subset["flops"]), np.log10(subset["emissions"])
        coef = np.polyfit(log_x, log_y, 1)
        trend_y = np.polyval(coef, log_x)
        order = np.argsort(subset["flops"])
        plt.plot(subset["flops"].values[order], 10**trend_y[order], linewidth=2,
                 label=f"{task} fit (slope={coef[0]:.2f})")
plt.xscale("log"); plt.yscale("log")
plt.xlabel("FLOPs")
plt.ylabel("Energy Consumption (kWh)")
plt.title("FLOPs vs Energy (Grouped Fits)", fontsize=14, fontweight="normal")
plt.legend()
plt.tight_layout()
plt.savefig("flops_grouped.png", dpi=300)

# FLOPs vs Energy (Ideal Line)
plt.figure(figsize=(8,6))
for task in ["inference-short", "inference-long", "training-b2-s128"]:
    subset = df[df["task"] == task]
    if len(subset) > 2:
        plt.scatter(subset["flops"], subset["emissions"], alpha=0.6, label=f"{task} data")
        log_x, log_y = np.log10(subset["flops"]), np.log10(subset["emissions"])
        coef = np.polyfit(log_x, log_y, 1)
        trend_y = np.polyval(coef, log_x)
        order = np.argsort(subset["flops"])
        plt.plot(subset["flops"].values[order], 10**trend_y[order], linewidth=2,
                 label=f"{task} fit (slope={coef[0]:.2f})")
# ç†æƒ³ slope=1
x_ref = np.logspace(9, 14, 100)
y_ref = 1e-4 * (x_ref / x_ref[0])
plt.plot(x_ref, y_ref, "--", color="gray", linewidth=2, label="Ideal slope=1")
plt.xscale("log"); plt.yscale("log")
plt.xlabel("FLOPs")
plt.ylabel("Energy Consumption (kWh)")
plt.title("FLOPs vs Energy (Ideal Line)", fontsize=14, fontweight="normal")
plt.legend()
plt.tight_layout()
plt.savefig("flops_ideal.png", dpi=300)

# CPU vs GPU
plt.figure(figsize=(8,6))
subset = df[df["task"] == "inference-short"]
models = subset["model"].unique()
for i, model in enumerate(models):
    model_data = subset[subset["model"] == model]
    cpu_val = model_data[model_data["device"] == "CPU"]["emissions"].mean()
    gpu_val = model_data[model_data["device"] == "GPU"]["emissions"].mean()
    if not pd.isna(cpu_val):
        plt.bar(i-0.2, cpu_val, width=0.4, color="red", alpha=0.7, label="CPU" if i==0 else "")
    if not pd.isna(gpu_val):
        plt.bar(i+0.2, gpu_val, width=0.4, color="blue", alpha=0.7, label="GPU" if i==0 else "")
plt.xticks(range(len(models)), models, rotation=30)
plt.yscale("log")
plt.ylabel("Energy Consumption (kWh)")
plt.title("CPU vs GPU Energy Comparison", fontsize=14, fontweight="normal")
plt.legend()
plt.tight_layout()
plt.savefig("cpu_vs_gpu.png", dpi=300)

# æ¨¡å‹å‚æ•°è§„æ¨¡ï¼ˆä¼°ç®—å€¼ï¼‰
param_counts = {
    "distilgpt2": 82e6,
    "gpt2": 124e6,
    "gpt2-medium": 355e6,
    "gpt2-large": 774e6,
    "gpt2-xl": 1.5e9,
    "tiiuae/falcon-7b": 7e9,
}

# æ–°å¢åˆ— 'params'
df["params"] = df["model"].map(param_counts)

import json
import pandas as pd

# ä»æ—¥å¿—è¯»å–
records = []
with open("logs/test_llm.jsonl", "r") as f:
    for line in f:
        try:
            records.append(json.loads(line.strip()))
        except:
            continue

df_log = pd.DataFrame(records)

# å¦‚æœæ²¡æœ‰ device å­—æ®µï¼Œç”¨æ¨¡å‹åæ¨æ–­
if "device" not in df_log.columns:
    def assign_device(model):
        big_models = ["gpt2-large", "gpt2-xl", "tiiuae/falcon-7b", "meta-llama/Llama-2-7b-hf"]
        if any(b in model for b in big_models):
            return "GPU"
        else:
            return "CPU"
    df_log["device"] = df_log["model"].apply(assign_device)

# ä¿ç•™ä¸»è¦å­—æ®µ
cols_to_keep = ["model", "task", "flops", "accuracy", "emissions", "device", "timestamp"]
for col in cols_to_keep:
    if col not in df_log.columns:
        df_log[col] = None

df_log = df_log[cols_to_keep]

# ä¿å­˜æ–° leaderboard
df_log.to_csv("leaderboard_fixed.csv", index=False)

print("âœ… å·²é‡å»º leaderboard_fixed.csvï¼ŒCPU/GPU å·²åŒºåˆ†")
print(df_log["device"].value_counts())

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# è¯»å–æ•°æ®
df = pd.read_csv("leaderboard_fixed.csv")

# åªçœ‹ inference-short
df_filtered = df[df["task"]=="inference-short"]

# æŒ‰æ¨¡å‹+è®¾å¤‡åˆ†ç»„
avg_energy = df_filtered.groupby(["model","device"])["emissions"].mean().unstack(fill_value=np.nan)

# æŒ‡å®šæ¨¡å‹é¡ºåºï¼ˆæŠŠ GPT ç³»åˆ—æ”¾ä¸€èµ·ï¼ŒFalcon æ”¾æœ€åï¼‰
model_order = ["distilgpt2","gpt2","gpt2-medium","gpt2-large","gpt2-xl","tiiuae/falcon-7b"]
avg_energy = avg_energy.reindex(model_order)

# ç”»å›¾
x = np.arange(len(avg_energy))
width = 0.35

plt.figure(figsize=(8,6))
plt.bar(x - width/2, avg_energy["CPU"], width, color="red", label="CPU")
plt.bar(x + width/2, avg_energy["GPU"], width, color="blue", label="GPU")

plt.xticks(x, avg_energy.index, rotation=45)
plt.yscale("log")
plt.ylabel("Energy Consumption (kWh)")
plt.title("CPU vs GPU Energy Comparison (Inference-Short)", fontsize=14, fontweight="normal")
plt.legend()
plt.tight_layout()
plt.savefig("cpu_vs_gpu_aligned.png", dpi=300)
plt.show()

# åªé€‰ GPU çš„æ•°æ®
df_gpu = df[df["device"] == "GPU"]

plt.figure(figsize=(8,6))
colors = {"inference-short": "blue", "inference-long": "orange"}
for task, color in colors.items():
    subset = df_gpu[df_gpu["task"] == task]
    if len(subset) > 2:
        plt.scatter(subset["flops"], subset["emissions"], color=color, alpha=0.6, label=f"{task} data")
        log_x, log_y = np.log10(subset["flops"]), np.log10(subset["emissions"])
        coef = np.polyfit(log_x, log_y, 1)
        trend_y = np.polyval(coef, log_x)
        order = np.argsort(subset["flops"])
        plt.plot(subset["flops"].values[order], 10**trend_y[order], color=color, linewidth=2,
                 label=f"{task} fit (slope={coef[0]:.2f})")

# ç†æƒ³ slope=1
x_ref = np.logspace(9, 14, 100)
y_ref = 1e-4 * (x_ref / x_ref[0])
plt.plot(x_ref, y_ref, "--", color="gray", linewidth=2, label="Ideal slope=1")

plt.xscale("log"); plt.yscale("log")
plt.xlabel("FLOPs")
plt.ylabel("Energy Consumption (kWh)")
plt.title("FLOPs vs Energy (GPU only)", fontsize=14, fontweight="normal")
plt.legend()
plt.tight_layout()
plt.savefig("flops_vs_energy_gpu.png", dpi=300)
plt.show()

plt.figure(figsize=(8,6))

# GPT-2 ç³»åˆ—
for task, marker in {"inference-short":"o", "inference-long":"s"}.items():
    subset = df_focus[(df_focus["model"].isin(gpt2_models)) & (df_focus["task"]==task)].sort_values("flops")
    plt.plot(subset["flops"], subset["emissions"],
             marker=marker, linestyle="-", color="blue", markersize=6,
             label=f"GPT-2 {task}")

# Falcon-7B
for task, marker in {"inference-short":"o", "inference-long":"s"}.items():
    falcon = df_focus[(df_focus["model"]=="tiiuae/falcon-7b") & (df_focus["task"]==task)]
    if len(falcon) > 0:
        plt.scatter(falcon["flops"], falcon["emissions"],
                    color="orange", edgecolor="black", marker=marker, s=120,
                    label=f"Falcon-7B {task}")

plt.xscale("log"); plt.yscale("log")
plt.xlabel("Model Parameters / FLOPs (log scale)")
plt.ylabel("Energy Consumption (kWh, log scale)")
plt.title("Scaling Law: GPT-2 vs Falcon-7B", fontsize=14, fontweight="normal")
plt.legend()
plt.tight_layout()
plt.savefig("scaling_gpt2_vs_falcon_final.png", dpi=300)
plt.show()

!pip install transformers bitsandbytes pynvml accelerate matplotlib

import torch
from torch.utils.data import DataLoader
from transformers import AutoModelForCausalLM, AutoTokenizer, Adafactor
from torch.optim import AdamW
import bitsandbytes as bnb
import pynvml
import time, csv
import matplotlib.pyplot as plt


# ----------------------------
# GPU åŠŸè€—ç›‘æ§
# ----------------------------
def init_nvml():
    pynvml.nvmlInit()
    handle = pynvml.nvmlDeviceGetHandleByIndex(0)
    return handle

def get_power(handle):
    return pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # W


# ----------------------------
# è®­ç»ƒè¿‡ç¨‹
# ----------------------------
def train(args):
    tokenizer = AutoTokenizer.from_pretrained(args.model)
    if tokenizer.pad_token is None:  # âœ… è§£å†³ GPT2 æ²¡æœ‰ pad_token çš„é—®é¢˜
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        args.model,
        torch_dtype=torch.float32 if args.precision == "fp32" else torch.bfloat16,
        device_map="auto"
    )

    texts = ["Hello world!"] * 256
    encodings = tokenizer(texts, return_tensors="pt", padding=True, truncation=True)
    dataset = torch.utils.data.TensorDataset(encodings["input_ids"], encodings["attention_mask"])
    dataloader = DataLoader(dataset, batch_size=4)

    # é€‰æ‹©ä¼˜åŒ–å™¨
    if args.optimizer == "adam":
        optimizer = AdamW(model.parameters(), lr=5e-5)
    elif args.optimizer == "adafactor":
        optimizer = Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)
    elif args.optimizer == "lamb":
        optimizer = bnb.optim.LAMB(model.parameters(), lr=5e-5)
    else:
        raise ValueError(f"Unsupported optimizer: {args.optimizer}")

    handle = init_nvml()
    total_energy = 0.0

    log_file = f"log_train_{args.model}_{args.optimizer}_{args.precision}.csv"
    with open(log_file, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["step", "loss", "power_watt", "energy_joule"])

        model.train()
        for step, batch in enumerate(dataloader):
            inputs = {
                "input_ids": batch[0].to(model.device),
                "attention_mask": batch[1].to(model.device),
                "labels": batch[0].to(model.device),
            }

            power = get_power(handle)
            start = time.time()

            outputs = model(**inputs)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            duration = time.time() - start
            total_energy += power * duration

            writer.writerow([step, loss.item(), power, total_energy])

            if step % 10 == 0:
                print(f"[Train] Step {step} | Loss {loss.item():.4f} | Power {power:.2f} W | Energy {total_energy:.2f} J")

    print(f"è®­ç»ƒå®Œæˆï¼Œæ€»èƒ½è€—: {total_energy:.2f} J")
    return model, tokenizer


# ----------------------------
# æ¨ç†æµ‹è¯•
# ----------------------------
def inference(args, model, tokenizer):
    handle = init_nvml()
    total_energy = 0.0
    total_tokens = 0

    prompts = ["The quick brown fox jumps over the lazy dog."] * 10

    log_file = f"log_infer_{args.model}_{args.optimizer}_{args.precision}.csv"
    with open(log_file, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["sample_id", "power_watt", "duration_sec", "energy_joule", "tokens"])

        for i, text in enumerate(prompts):
            inputs = tokenizer(text, return_tensors="pt").to(model.device)

            power = get_power(handle)
            start = time.time()

            outputs = model.generate(**inputs, max_new_tokens=50)
            duration = time.time() - start
            tokens = outputs.shape[1] - inputs["input_ids"].shape[1]

            energy = power * duration
            total_energy += energy
            total_tokens += tokens

            writer.writerow([i, power, duration, energy, tokens])
            print(f"[Infer] Sample {i} | Tokens {tokens} | Power {power:.2f} W | Energy {energy:.2f} J")

    avg_j_per_token = total_energy / total_tokens
    print(f"æ¨ç†å®Œæˆ: æ€»èƒ½è€— {total_energy:.2f} J, æ€»tokens {total_tokens}, å¹³å‡ {avg_j_per_token:.6f} J/token")

class Args:
    model = "gpt2"
    precision = "fp32"   # fp32 / bf16
    optimizer = "adam"   # adam / adafactor / lamb

args = Args()
args.model = "gpt2"
args.precision = "fp32"
args.optimizer = "adam"

model, tokenizer = train(args)
inference(args, model, tokenizer)

results = []

for optimizer in ["adam", "adafactor"]:
    for precision in ["fp32", "bf16"]:
        args = Args()
        args.model = "gpt2"
        args.optimizer = optimizer
        args.precision = precision
        print(f"\n==== Running {optimizer} {precision} ====")
        model, tokenizer = train(args)
        inference(args, model, tokenizer)

import pandas as pd
import glob

# æ”¶é›†è®­ç»ƒæ—¥å¿—
train_logs = glob.glob("log_train_*.csv")
infer_logs = glob.glob("log_infer_*.csv")

summary = []

for log in train_logs:
    df = pd.read_csv(log)
    total_energy = df["energy_joule"].iloc[-1]
    parts = log.replace("log_train_", "").replace(".csv", "").split("_")
    model, optimizer, precision = parts[0], parts[1], parts[2]
    summary.append({
        "Model": model,
        "Optimizer": optimizer,
        "Precision": precision,
        "Train Energy (J)": total_energy
    })

for log in infer_logs:
    df = pd.read_csv(log)
    total_energy = df["energy_joule"].sum()
    tokens = df["tokens"].sum()
    avg_j_token = total_energy / tokens
    parts = log.replace("log_infer_", "").replace(".csv", "").split("_")
    model, optimizer, precision = parts[0], parts[1], parts[2]
    for row in summary:
        if row["Optimizer"] == optimizer and row["Precision"] == precision:
            row["Infer Energy (J)"] = total_energy
            row["Avg J/token"] = avg_j_token

df_summary = pd.DataFrame(summary)
print(df_summary)

# è¾“å‡º LaTeX è¡¨æ ¼
print(df_summary.to_latex(index=False, float_format="%.4f"))

import pandas as pd
import matplotlib.pyplot as plt

def plot_energy(log_files, title="Training Energy Consumption Curve"):
    fig, ax = plt.subplots()
    for log in log_files:
        df = pd.read_csv(log)
        label = log.replace("log_", "").replace(".csv", "")
        ax.plot(df["step"], df["energy_joule"], label=label)
    ax.set_xlabel("Training Step")
    ax.set_ylabel("Energy (J)")
    ax.set_title(title)   # âœ… English title
    ax.legend(title="Configuration")  # âœ… English legend title
    plt.show()

# Example call
plot_energy([
    "log_train_gpt2_adam_fp32.csv",
    "log_train_gpt2_adam_bf16.csv",
    "log_train_gpt2_adafactor_fp32.csv",
    "log_train_gpt2_adafactor_bf16.csv"
], title="GPT-2 Training Energy Consumption (Optimizer Ã— Precision)")
import pandas as pd
import matplotlib.pyplot as plt

import pandas as pd
import matplotlib.pyplot as plt

# Example summary data
data = {
    "Optimizer": ["adam", "adam", "adafactor", "adafactor"],
    "Precision": ["bf16", "fp32", "fp32", "bf16"],
    "Avg J/token": [0.591230, 0.652791, 0.611825, 0.574222]
}
df_summary = pd.DataFrame(data)

# Pivot table for grouped bar plot
df_pivot = df_summary.pivot(index="Optimizer", columns="Precision", values="Avg J/token")

# Plot grouped bar chart
ax = df_pivot.plot(kind="bar", figsize=(6,5))
ax.set_title("GPT-2 Inference Energy per Token (Optimizer Ã— Precision)")
ax.set_ylabel("Energy (J/token)")
ax.set_xlabel("Optimizer")
plt.xticks(rotation=0)
plt.legend(title="Precision")
plt.tight_layout()
plt.show()

!pip install transformers bitsandbytes pynvml accelerate fvcore

import torch, time, csv
import pynvml
from transformers import AutoModelForCausalLM, AutoTokenizer
from fvcore.nn import FlopCountAnalysis

from huggingface_hub import login


!pip install transformers bitsandbytes pynvml accelerate fvcore matplotlib

import os, torch, time, csv, pynvml
from transformers import AutoModelForCausalLM, AutoTokenizer
from fvcore.nn import FlopCountAnalysis
from huggingface_hub import login
import matplotlib.pyplot as plt
import pandas as pd

# ----------------------------
# ç™»å½• HuggingFaceï¼ˆå®‰å…¨ï¼šä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
# åœ¨ Colab å·¦è¾¹æ : âš™ Settings â†’ Secrets â†’ æ·»åŠ  HF_TOKEN
# ç„¶åè¿™é‡Œä¼šè‡ªåŠ¨è¯»å–
# ----------------------------


# ----------------------------
from transformers import AutoModelForCausalLM, AutoTokenizer
import pynvml, torch, time

# ----------------------------
# GPU èƒ½è€—ç›‘æ§
# ----------------------------
def init_nvml():
    pynvml.nvmlInit()
    return pynvml.nvmlDeviceGetHandleByIndex(0)

def get_power(handle):
    return pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # W


# ----------------------------
# æ¨ç†èƒ½è€—æµ‹é‡ (å¿«é€Ÿç‰ˆ)
# ----------------------------
def measure_inference_fast(model_name, seq_len=128, n_samples=2):
    print(f"\n===== Running {model_name} (fast mode) =====")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # âœ… åŠç²¾åº¦åŠ è½½ï¼Œå‡å°æ˜¾å­˜å ç”¨
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        torch_dtype=torch.bfloat16
    )
    model.eval()

    handle = init_nvml()
    total_energy, total_tokens = 0.0, 0

    for i in range(n_samples):
        text = "The quick brown fox jumps over the lazy dog."
        inputs = tokenizer(text, return_tensors="pt").to(model.device)

        power = get_power(handle)
        start = time.time()
        outputs = model.generate(**inputs, max_new_tokens=seq_len)
        duration = time.time() - start
        tokens = outputs.shape[1] - inputs["input_ids"].shape[1]

        energy = power * duration
        total_energy += energy
        total_tokens += tokens
        print(f"[{model_name}] Sample {i} | {tokens} tokens | {energy:.2f} J")

    avg_j_per_token = total_energy / total_tokens
    print(f"[{model_name}] Total: {total_tokens} tokens, {total_energy:.2f} J, {avg_j_per_token:.6f} J/token")

    return avg_j_per_token, total_tokens


# ----------------------------
# è·‘ Mixtral-8x7B (MoE)
# ----------------------------
model_name = "mistralai/Mixtral-8x7B-Instruct-v0.1"
energy, tokens = measure_inference_fast(model_name)

print("\n===== Final Result =====")
print(f"Model: {model_name}, Energy/token: {energy:.6f} J, Tokens: {tokens}")